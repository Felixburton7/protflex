Working Directory: /home/s_felix/drFelix/src/protflex

File Structure:
.
├── AI_context.sh
├── AI_context.txt
├── cli
│   ├── cli.py
│   ├── commands
│   │   ├── evaluate.py
│   │   ├── __init__.py
│   │   ├── predict.py
│   │   └── train.py
│   └── __init__.py
├── config
│   ├── config.py
│   ├── config.yaml
│   └── __init__.py
├── data
│   ├── data_loader.py
│   ├── dataset.py
│   ├── datasets.py
│   └── __init__.py
├── __init__.py
├── __main__.py
├── models
│   ├── cnn_models.py
│   ├── __init__.py
│   ├── layers.py
│   └── loss.py
├── quick_changes_script.py
├── README.md
├── setup.py
├── training
│   ├── __init__.py
│   ├── trainer.py
│   └── validators.py
└── utils
    ├── file_utils.py
    ├── __init__.py
    ├── logging_utils.py
    └── visualization.py

7 directories, 31 files

Contents of Relevant Files Below (Ignoring Binary Files):
---------------------------------------------------------
===== FILE: config/config.yaml =====
input:
  data_dir: ~/drFelix/data_15
  voxel_dir: processed/voxelized_output
  rmsf_dir: interim/per-residue-rmsf
  temperature: 320  # Can be a specific temperature (320, 348, 379, 413, 450) or "average"
  domain_ids: []  # Empty means process all domains
  use_metadata: true
  metadata_fields:
    - resname_encoded
    - normalized_resid
    - secondary_structure_encoded
    - core_exterior_encoded
    - relative_accessibility

output:
  base_dir: ~/protflex_results
  model_dir: models
  results_dir: results
  visualizations_dir: visualizations
  log_file: protflex.log

model:
  # Model architecture options: "protflex_cnn", "dilated_resnet3d", "multipath_rmsf_net"
  architecture: multipath_rmsf_net
  input_channels: 5  # C, N, O, CA, CB channels
  channel_growth_rate: 1.5
  num_residual_blocks: 4
  use_multiscale: true
  use_bottleneck: true
  dropout_rate: 0.3
  include_metadata: true
  base_filters: 32  # Base number of filters for convolutional layers

training:
  batch_size: 32
  num_epochs: 100
  learning_rate: 0.001
  weight_decay: 1e-5
  early_stopping_patience: 10
  # Options: "reduce_on_plateau", "cosine_annealing", "step", "exponential"
  lr_scheduler: reduce_on_plateau
  lr_scheduler_params:
    factor: 0.5
    patience: 5
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  random_seed: 42
  # Augmentation options
  use_augmentation: true
  augmentation_params:
    rotation_prob: 0.5
    rotation_angles: [90, 180, 270]
    flip_prob: 0.3
    noise_prob: 0.2
    noise_scale: 0.05

logging:
  level: INFO
  console_level: INFO
  file_level: DEBUG
  show_progress_bars: true

visualization:
  plot_loss: true
  plot_predictions: true
  plot_residue_type_analysis: true
  plot_error_distribution: true
  save_format: png
  dpi: 300
  plot_correlation: true
  plot_amino_acid_performance: true
  max_scatter_points: 1000  # Limit for scatter plots to prevent overcrowding
===== FILE: config/config.py =====
"""
Configuration handling for ProtFlex.

This module loads and validates YAML configuration files for the ProtFlex pipeline.
"""

import os
import yaml
import logging
from typing import Dict, List, Any, Optional, Union

# Default configuration settings
DEFAULT_CONFIG = {
    "input": {
        "data_dir": os.path.expanduser("~/data"),
        "voxel_dir": "processed/voxelized_output",
        "rmsf_dir": "interim/per-residue-rmsf",
        "temperature": 320,
        "domain_ids": [],  # Empty means process all domains
        "use_metadata": True,
        "metadata_fields": ["resname_encoded", "normalized_resid", "secondary_structure_encoded"]
    },
    "output": {
        "base_dir": os.path.expanduser("~/protflex_results"),
        "model_dir": "models",
        "results_dir": "results",
        "visualizations_dir": "visualizations",
        "log_file": "protflex.log"
    },
    "model": {
        "architecture": "protflex_cnn",
        "input_channels": 5,  # C, N, O, CA, CB channels
        "channel_growth_rate": 1.5,
        "num_residual_blocks": 4,
        "use_multiscale": True,
        "use_bottleneck": True,
        "dropout_rate": 0.2,
        "include_metadata": True
    },
    "training": {
        "batch_size": 32,
        "num_epochs": 100,
        "learning_rate": 0.001,
        "weight_decay": 1e-5,
        "early_stopping_patience": 10,
        "lr_scheduler": "reduce_on_plateau",
        "lr_scheduler_params": {
            "factor": 0.5,
            "patience": 5
        },
        "train_split": 0.7,
        "val_split": 0.15,
        "test_split": 0.15,
        "random_seed": 42
    },
    "logging": {
        "level": "INFO",
        "console_level": "INFO",
        "file_level": "DEBUG",
        "show_progress_bars": True
    },
    "visualization": {
        "plot_loss": True,
        "plot_predictions": True,
        "plot_residue_type_analysis": True,
        "plot_error_distribution": True,
        "save_format": "png",
        "dpi": 300
    }
}

def load_config(config_path: str) -> Dict[str, Any]:
    """
    Load configuration from YAML file with fallback to default values.
    
    Args:
        config_path: Path to YAML configuration file
        
    Returns:
        Dictionary containing merged configuration
    """
    logger = logging.getLogger(__name__)
    
    # Start with default configuration
    config = DEFAULT_CONFIG.copy()
    
    # Load user configuration if exists
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r') as f:
                user_config = yaml.safe_load(f)
                if user_config:
                    # Deep merge configs
                    deep_merge(config, user_config)
            logger.info(f"Loaded configuration from {config_path}")
        except Exception as e:
            logger.error(f"Error loading configuration from {config_path}: {e}")
            logger.info("Using default configuration")
    else:
        logger.warning(f"Configuration file {config_path} not found. Using default configuration.")
    
    # Process and validate configuration
    config = process_config(config)
    
    return config

def process_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process and validate configuration:
    - Expand user paths
    - Set derived values
    - Validate settings
    
    Args:
        config: Raw configuration dictionary
        
    Returns:
        Processed configuration dictionary
    """
    # Expand user paths
    for section in ["input", "output"]:
        for key in config[section]:
            if isinstance(config[section][key], str) and "~" in config[section][key]:
                config[section][key] = os.path.expanduser(config[section][key])
    
    # Ensure all required directories are present in output configuration
    for dir_key in ["model_dir", "results_dir", "visualizations_dir"]:
        if dir_key in config["output"]:
            # Make the path relative to base_dir if not absolute
            if not os.path.isabs(config["output"][dir_key]):
                config["output"][dir_key] = os.path.join(
                    config["output"]["base_dir"], 
                    config["output"][dir_key]
                )
    
    # Process log file path
    if "log_file" in config["output"] and not os.path.isabs(config["output"]["log_file"]):
        config["output"]["log_file"] = os.path.join(
            config["output"]["base_dir"],
            config["output"]["log_file"]
        )
    
    # Set model metadata parameters based on input configuration
    if config["model"]["include_metadata"] and config["input"]["use_metadata"]:
        # Calculate metadata_features dimension based on the metadata fields
        metadata_features = 0
        if "resname_encoded" in config["input"]["metadata_fields"]:
            metadata_features += 20  # 20 standard amino acids
        if "normalized_resid" in config["input"]["metadata_fields"]:
            metadata_features += 1
        if "secondary_structure_encoded" in config["input"]["metadata_fields"]:
            metadata_features += 3  # 3 classes: helix, sheet, loop
        if "core_exterior_encoded" in config["input"]["metadata_fields"]:
            metadata_features += 2  # core or exterior
        if "relative_accessibility" in config["input"]["metadata_fields"]:
            metadata_features += 1
        
        config["model"]["metadata_features"] = metadata_features
    else:
        config["model"]["include_metadata"] = False
        config["model"]["metadata_features"] = 0
    
    return config

def deep_merge(target: Dict[str, Any], source: Dict[str, Any]) -> Dict[str, Any]:
    """
    Recursively merge source dict into target dict.
    
    Args:
        target: Target dictionary to merge into
        source: Source dictionary to merge from
        
    Returns:
        Merged dictionary (target is modified in-place)
    """
    for key, value in source.items():
        if key in target and isinstance(target[key], dict) and isinstance(value, dict):
            deep_merge(target[key], value)
        else:
            target[key] = value
    return target

def create_example_config() -> None:
    """
    Create an example configuration file.
    """
    with open('config.yaml.example', 'w') as f:
        yaml.dump(DEFAULT_CONFIG, f, default_flow_style=False, sort_keys=False)

def get_temperature_dir(temperature: Union[int, str]) -> str:
    """
    Get the directory name for a temperature value.
    
    Args:
        temperature: Temperature value or "average"
        
    Returns:
        Directory name for the temperature
    """
    if temperature == "average":
        return "average"
    else:
        return str(temperature)

if __name__ == "__main__":
    # Generate example configuration file when run directly
    create_example_config()
    print("Created example configuration file: config.yaml.example")
===== FILE: config/__init__.py =====
"""
Configuration module for ProtFlex.

This module handles loading and processing of configuration parameters.
"""

from .config import load_config, process_config, create_example_config, get_temperature_dir

===== FILE: cli/cli.py =====

#!/usr/bin/env python3
"""
Command-line interface for ProtFlex.

This module provides the main entry point for the ProtFlex CLI application.
"""

import os
import sys
import argparse
import logging
from typing import List, Optional

from protflex.cli.commands import train, predict, evaluate
from protflex.config import config
from protflex.utils.logging_utils import setup_logging

def main(args: Optional[List[str]] = None) -> int:
    """
    Main entry point for the ProtFlex CLI.

    Args:
        args: Command line arguments (if None, sys.argv[1:] is used)

    Returns:
        Exit code
    """
    # Create the main parser
    parser = argparse.ArgumentParser(
        description='ProtFlex: Deep learning for protein flexibility prediction',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Add global arguments
    parser.add_argument('--config', '-c', type=str, default='config.yaml',
                      help='Path to configuration file')
    parser.add_argument('--verbose', '-v', action='store_true',
                      help='Enable verbose output')
    parser.add_argument('--quiet', '-q', action='store_true',
                      help='Suppress all output except errors')

    # Create subparsers for different commands
    subparsers = parser.add_subparsers(dest='command', help='Command to execute')

    # Add command subparsers
    train.add_subparser(subparsers)
    predict.add_subparser(subparsers)
    evaluate.add_subparser(subparsers)

    # Parse arguments
    parsed_args = parser.parse_args(args)

    # If no command specified, show help and exit
    if not parsed_args.command:
        parser.print_help()
        return 1

    # Load configuration
    try:
        cfg = config.load_config(parsed_args.config)
    except Exception as e:
        print(f"Error loading configuration: {e}")
        return 1

    # Configure logging based on verbosity
    log_level = logging.INFO
    if parsed_args.verbose:
        log_level = logging.DEBUG
    elif parsed_args.quiet:
        log_level = logging.ERROR

    logger = setup_logging(cfg, log_level)

    # Execute the specified command
    try:
        if parsed_args.command == 'train':
            return train.run(parsed_args, cfg)
        elif parsed_args.command == 'predict':
            return predict.run(parsed_args, cfg)
        elif parsed_args.command == 'evaluate':
            return evaluate.run(parsed_args, cfg)
        else:
            logger.error(f"Unknown command: {parsed_args.command}")
            return 1
    except Exception as e:
        logger.exception(f"Error executing command {parsed_args.command}: {e}")
        return 1

if __name__ == '__main__':
    sys.exit(main())

===== FILE: cli/commands/predict.py =====
"""
Prediction command for ProtFlex CLI.

This module provides the command to predict protein flexibility using a trained model.
"""

import os
import argparse
import logging
import json
from typing import Dict, Any, List, Tuple

import torch
import numpy as np
import pandas as pd
import h5py
from tqdm import tqdm

from protflex.models import cnn_models
from protflex.data.data_loader import RMSFDataset
from protflex.utils.visualization import plot_rmsf_profiles

logger = logging.getLogger(__name__)

def add_subparser(subparsers: argparse._SubParsersAction) -> argparse.ArgumentParser:
    """
    Add a subparser for the predict command.

    Args:
        subparsers: Subparsers object

    Returns:
        Parser for the predict command
    """
    parser = subparsers.add_parser(
        'predict',
        help='Make RMSF predictions using a trained model',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Input arguments
    parser.add_argument('--model-path', type=str, required=True,
                      help='Path to trained model checkpoint')
    parser.add_argument('--data-dir', type=str,
                      help='Directory containing data (overrides config)')
    parser.add_argument('--domain-ids', type=str, nargs='+',
                      help='List of domain IDs to predict (if not specified, all available domains will be used)')

    # Output arguments
    parser.add_argument('--output-dir', type=str,
                      help='Directory to save prediction results (overrides config)')
    parser.add_argument('--plot', action='store_true',
                      help='Generate RMSF profile plots')
    parser.add_argument('--compare-actual', action='store_true',
                      help='Compare predictions with actual RMSF values if available')
    parser.add_argument('--cpu', action='store_true',
                      help='Force using CPU even if GPU is available')

    return parser


def predict_domain(
    model: torch.nn.Module,
    domain_id: str,
    voxel_dir: str,
    temperature: str,
    device: torch.device,
    rmsf_scaler = None
) -> Tuple[List[int], List[float], List[str]]:
    """
    Predict RMSF values for a domain with robust error handling.

    Args:
        model: Trained model
        domain_id: Domain ID
        voxel_dir: Directory containing voxelized data
        temperature: Temperature value
        device: Device to run the model on
        rmsf_scaler: Scaler for normalizing RMSF values

    Returns:
        Tuple of (residue_ids, predicted_rmsf_values, residue_names)
    """
    # Find voxel files for the domain
    domain_voxel_dir = os.path.join(voxel_dir, domain_id)
    
    # Check for temperature-specific directory
    temp_dir = os.path.join(domain_voxel_dir, str(temperature))
    if os.path.exists(temp_dir):
        domain_voxel_dir = temp_dir
    else:
        # Check for PDB directory as a fallback
        pdb_dir = os.path.join(domain_voxel_dir, "pdb")
        if os.path.exists(pdb_dir):
            domain_voxel_dir = pdb_dir
    
    if not os.path.exists(domain_voxel_dir):
        logger.error(f"Voxel directory not found for domain {domain_id} at temperature {temperature}")
        return [], [], []
    
    voxel_files = [f for f in os.listdir(domain_voxel_dir) if f.endswith('.hdf5')]
    
    if not voxel_files:
        logger.error(f"No voxel files found for domain {domain_id} in {domain_voxel_dir}")
        return [], [], []
    
    residue_ids = []
    predicted_rmsf = []
    residue_names = []
    
    # Process each voxel file
    for voxel_file in tqdm(voxel_files, desc=f"Predicting {domain_id}"):
        try:
            voxel_path = os.path.join(domain_voxel_dir, voxel_file)
            
            # Extract residue ID from filename
            residue_id = extract_residue_id_from_filename(voxel_path)
            
            if residue_id is None:
                logger.warning(f"Could not extract residue ID from {voxel_path}, skipping")
                continue
            
            # Load voxel data
            voxel_data, metadata = load_voxel_data(voxel_path)
            
            if voxel_data is None:
                logger.warning(f"Failed to load voxel data from {voxel_path}, skipping")
                continue
            
            # Check data shape for compatibility with model
            expected_channels = 5  # C, N, O, CA, CB channels
            if voxel_data.shape[0] != expected_channels:
                logger.warning(f"Unexpected channel count in {voxel_path}: expected {expected_channels}, got {voxel_data.shape[0]}")
                continue
            
            # Convert to tensor
            voxel_tensor = torch.from_numpy(voxel_data).float().unsqueeze(0).to(device)
            
            # Get residue name from metadata
            residue_name = "UNK"
            if "resname" in metadata:
                residue_name = metadata["resname"]
            elif "residue_name" in metadata:
                residue_name = metadata["residue_name"]
            
            # Forward pass through model
            with torch.no_grad():
                # Handle models that expect metadata
                if getattr(model, 'include_metadata', False):
                    # Create metadata tensor
                    metadata_size = getattr(model, 'metadata_features', 0)
                    metadata_tensor = torch.zeros(1, metadata_size, device=device)
                    
                    # Fill metadata tensor if possible
                    current_idx = 0
                    
                    # One-hot encode residue name if available
                    if metadata_size >= 20 and "resname" in metadata:
                        # Get amino acid index (simple mapping for standard amino acids)
                        aa_dict = {"ALA": 0, "ARG": 1, "ASN": 2, "ASP": 3, "CYS": 4,
                                  "GLN": 5, "GLU": 6, "GLY": 7, "HIS": 8, "ILE": 9,
                                  "LEU": 10, "LYS": 11, "MET": 12, "PHE": 13, "PRO": 14,
                                  "SER": 15, "THR": 16, "TRP": 17, "TYR": 18, "VAL": 19}
                        
                        aa_idx = aa_dict.get(metadata["resname"], -1)
                        if aa_idx >= 0:
                            metadata_tensor[0, aa_idx] = 1.0
                    
                    # Pass both voxel data and metadata to the model
                    output = model(voxel_tensor, metadata_tensor)
                else:
                    output = model(voxel_tensor)
                
                # Get prediction
                pred = output.item()
                
                # Denormalize if scaler provided
                if rmsf_scaler is not None:
                    pred = rmsf_scaler.inverse_transform([[pred]])[0, 0]
                
                # Store results
                residue_ids.append(residue_id)
                predicted_rmsf.append(pred)
                residue_names.append(residue_name)
        
        except Exception as e:
            logger.error(f"Error processing voxel file {voxel_file}: {e}")
    
    # Check if we have any predictions
    if not residue_ids:
        logger.warning(f"No predictions generated for domain {domain_id}")
        return [], [], []
    
    # Sort by residue ID
    sorted_indices = np.argsort(residue_ids)
    residue_ids = [residue_ids[i] for i in sorted_indices]
    predicted_rmsf = [predicted_rmsf[i] for i in sorted_indices]
    residue_names = [residue_names[i] for i in sorted_indices]
    
    return residue_ids, predicted_rmsf, residue_names

def load_voxel_data(voxel_path: str) -> Tuple[Optional[np.ndarray], Optional[Dict[str, Any]]]:
    """
    Load voxel data from an HDF5 file with robust error handling.
    
    Args:
        voxel_path: Path to the voxel HDF5 file
        
    Returns:
        Tuple of (voxel_data, metadata) or (None, None) if loading fails
    """
    try:
        with h5py.File(voxel_path, 'r') as f:
            voxel_data = None
            metadata = {}
            
            # Extract metadata
            for key in f.attrs:
                metadata[key] = f.attrs[key]
            
            # Try different possible structures for voxel data
            
            # Option 1: Modern aposteriori format with "inputs" dataset
            if "inputs" in f:
                voxel_data = f["inputs"][:]
                
                # Get additional metadata from inputs group
                if hasattr(f["inputs"], "attrs"):
                    for key in f["inputs"].attrs:
                        metadata[f"input_{key}"] = f["inputs"].attrs[key]
            
            # Option 2: Legacy format with "voxel_data" dataset
            elif "voxel_data" in f:
                voxel_data = f["voxel_data"][:]
            
            # Option 3: Direct data in root dataset
            elif len(f.keys()) == 0 and isinstance(f, h5py.Dataset):
                voxel_data = f[:]
            
            # Option 4: Look for any dataset with 3D or 4D shape
            else:
                for key in f.keys():
                    if isinstance(f[key], h5py.Dataset):
                        dataset = f[key]
                        shape = dataset.shape
                        
                        if len(shape) >= 3:
                            voxel_data = dataset[:]
                            break
            
            # Extract metadata from metadata group if it exists
            if "metadata" in f:
                metadata_group = f["metadata"]
                
                # Extract attributes
                if hasattr(metadata_group, "attrs"):
                    for key in metadata_group.attrs:
                        metadata[key] = metadata_group.attrs[key]
            
            if voxel_data is None:
                logger.warning(f"Could not find voxel data in {voxel_path}")
                return None, metadata
            
            # Reshape if needed
            if len(voxel_data.shape) == 4:  # [batch, channels, height, width]
                voxel_data = voxel_data[0]  # Take the first batch item
            
            # Validate data shape
            if len(voxel_data.shape) != 3 and len(voxel_data.shape) != 4:
                logger.warning(f"Unexpected data shape in {voxel_path}: {voxel_data.shape}")
                return None, metadata
                
            return voxel_data, metadata
    
    except Exception as e:
        logger.error(f"Error loading voxel file {voxel_path}: {e}")
        return None, None
    
def extract_residue_id_from_filename(voxel_file: str) -> Optional[int]:
    """
    Extract residue ID from voxel filename using various patterns.
    
    Args:
        voxel_file: Path to voxel file
        
    Returns:
        Extracted residue ID or None if not found
    """
    file_name = os.path.basename(voxel_file)
    
    # Try several patterns to extract residue ID
    
    # Pattern 1: Look for _res{ID}_ or _residue{ID}_
    import re
    res_pattern = re.compile(r'_res(\d+)_|_residue(\d+)_')
    match = res_pattern.search(file_name)
    if match:
        # Get the matched group that's not None
        for group in match.groups():
            if group is not None:
                return int(group)
    
    # Pattern 2: Extract from file parts
    parts = file_name.split('_')
    
    # Look for numeric parts
    for i, part in enumerate(parts):
        if part.isdigit():
            # Check if previous part indicates this is a residue ID
            if i > 0 and (parts[i-1].lower() == 'res' or parts[i-1].lower() == 'residue'):
                return int(part)
    
    # Pattern 3: Try to extract from directory structure
    dir_path = os.path.dirname(voxel_file)
    dir_parts = dir_path.split(os.sep)
    
    # Look for numeric directory names
    for part in dir_parts:
        if part.isdigit():
            return int(part)
    
    # Pattern 4: Try to read ID from HDF5 file
    try:
        with h5py.File(voxel_file, 'r') as f:
            # Check metadata attributes
            if "metadata" in f and "resid" in f["metadata"].attrs:
                resid = f["metadata"].attrs["resid"]
                # Validate the residue ID
                try:
                    return int(resid)
                except (ValueError, TypeError):
                    logger.warning(f"Invalid residue ID in metadata.attrs.resid: {resid}")
            
            # Check top-level attributes
            if "resid" in f.attrs:
                resid = f.attrs["resid"]
                try:
                    return int(resid)
                except (ValueError, TypeError):
                    logger.warning(f"Invalid residue ID in attrs.resid: {resid}")
            
            # Check inputs dataset attributes
            if "inputs" in f and "resid" in f["inputs"].attrs:
                resid = f["inputs"].attrs["resid"]
                try:
                    return int(resid)
                except (ValueError, TypeError):
                    logger.warning(f"Invalid residue ID in inputs.attrs.resid: {resid}")
    except Exception as e:
        logger.error(f"Error reading residue ID from HDF5 file {voxel_file}: {e}")
    
    # If all extraction methods fail, return None
    logger.warning(f"Could not extract residue ID from filename: {voxel_file}")
    return None

def run(args: argparse.Namespace, config: Dict[str, Any]) -> int:
    """
    Run the predict command.

    Args:
        args: Command line arguments
        config: Configuration dictionary

    Returns:
        Exit code
    """
    # Override config with command-line arguments
    if args.data_dir:
        config['input']['data_dir'] = args.data_dir
    if args.output_dir:
        config['output']['base_dir'] = args.output_dir
    if args.domain_ids:
        config['input']['domain_ids'] = args.domain_ids

    # Set up directories
    data_dir = config['input']['data_dir']
    voxel_dir = os.path.join(data_dir, config['input']['voxel_dir'])
    rmsf_dir = os.path.join(data_dir, config['input']['rmsf_dir'])
    output_dir = config['output']['base_dir']
    results_dir = os.path.join(output_dir, config['output']['results_dir'])

    # Create output directories
    os.makedirs(results_dir, exist_ok=True)
    predictions_dir = os.path.join(results_dir, 'predictions')
    os.makedirs(predictions_dir, exist_ok=True)

    if args.plot:
        plots_dir = os.path.join(results_dir, 'plots')
        os.makedirs(plots_dir, exist_ok=True)

    # Load model checkpoint
    logger.info(f"Loading model from {args.model_path}")

    # Determine device (CPU/GPU)
    device = torch.device("cuda:0" if torch.cuda.is_available() and not args.cpu else "cpu")
    logger.info(f"Using device: {device}")

    try:
        checkpoint = torch.load(args.model_path, map_location=device)
        model_name = checkpoint.get('model_name', config['model']['architecture'])
        model_params = config['model'].copy()
        model_params.pop('architecture', None)

        # Create model
        model = cnn_models.create_model(model_name, model_params)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        model.eval()

        # Get temperature from checkpoint or config
        temperature = checkpoint.get('temperature', config['input']['temperature'])

        logger.info(f"Model loaded successfully. Using temperature: {temperature}")
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        return 1

    # Get domain IDs to process
    domain_ids = config['input']['domain_ids']
    if not domain_ids:
        # Find all domains with voxel data
        domain_ids = [d for d in os.listdir(voxel_dir)
                     if os.path.isdir(os.path.join(voxel_dir, d))]

    if not domain_ids:
        logger.error(f"No domains found in {voxel_dir}")
        return 1

    logger.info(f"Processing {len(domain_ids)} domains")

    # Process each domain
    all_results = []

    for domain_id in domain_ids:
        logger.info(f"Processing domain {domain_id}")

        # Predict RMSF values
        residue_ids, predicted_rmsf, residue_names = predict_domain(
            model, domain_id, voxel_dir, temperature, device)

        if not residue_ids:
            logger.warning(f"No predictions generated for domain {domain_id}")
            continue

        # Create DataFrame with predictions
        df = pd.DataFrame({
            'domain_id': domain_id,
            'resid': residue_ids,
            'resname': residue_names,
            f'predicted_rmsf_{temperature}': predicted_rmsf
        })

        # Compare with actual RMSF values if requested
        if args.compare_actual:
            if temperature == "average":
                actual_rmsf_file = os.path.join(rmsf_dir, "average", f"{domain_id}_total_average_rmsf.csv")
            else:
                actual_rmsf_file = os.path.join(rmsf_dir, str(temperature),
                                             f"{domain_id}_temperature_{temperature}_average_rmsf.csv")

            if os.path.exists(actual_rmsf_file):
                actual_df = pd.read_csv(actual_rmsf_file)

                # Determine RMSF column name
                if temperature == "average":
                    rmsf_col = "average_rmsf"
                else:
                    rmsf_col = f"rmsf_{temperature}"

                # Merge predictions with actual values
                df = df.merge(actual_df[['resid', rmsf_col]], on='resid', how='left')
                df.rename(columns={rmsf_col: f'actual_rmsf_{temperature}'}, inplace=True)

        # Save predictions to CSV
        output_file = os.path.join(predictions_dir, f"{domain_id}_predictions.csv")
        df.to_csv(output_file, index=False)
        logger.info(f"Saved predictions to {output_file}")

        # Create plot if requested
        if args.plot:
            if 'actual_rmsf' in df.columns.str.contains(f'actual_rmsf_{temperature}').any():
                actual_col = f'actual_rmsf_{temperature}'
                actual_values = df[actual_col].values
            else:
                actual_values = None

            plot_file = os.path.join(plots_dir, f"{domain_id}_rmsf_profile.png")
            plot_rmsf_profiles(
                domain_id=domain_id,
                residue_ids=df['resid'].values,
                actual_rmsf=actual_values if actual_values is not None else [],
                predicted_rmsf=df[f'predicted_rmsf_{temperature}'].values,
                residue_names=df['resname'].values,
                output_path=plot_file
            )
            logger.info(f"Saved RMSF profile plot to {plot_file}")

        all_results.append(df)

    # Combine all results
    if all_results:
        all_df = pd.concat(all_results, ignore_index=True)
        all_output_file = os.path.join(predictions_dir, f"all_domains_predictions.csv")
        all_df.to_csv(all_output_file, index=False)
        logger.info(f"Saved combined predictions to {all_output_file}")

    logger.info("Prediction completed successfully")
    return 0

===== FILE: cli/commands/train.py =====
"""
Training command for ProtFlex CLI.

This module provides the command to train protein flexibility prediction models.
"""

import os
import argparse
import logging
import json
from typing import Dict, Any, Optional

import torch
import numpy as np

from protflex.models import cnn_models
from protflex.models.loss import create_loss_function
from protflex.data.data_loader import create_data_loaders
from protflex.training.trainer import (
    RMSFTrainer,
    create_optimizer,
    create_scheduler
)
from protflex.utils.file_utils import ensure_dir, save_model_summary

logger = logging.getLogger(__name__)

def add_subparser(subparsers: argparse._SubParsersAction) -> argparse.ArgumentParser:
    """
    Add a subparser for the train command.

    Args:
        subparsers: Subparsers object

    Returns:
        Parser for the train command
    """
    parser = subparsers.add_parser(
        'train',
        help='Train a protein flexibility prediction model',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Data arguments
    parser.add_argument('--data-dir', type=str,
                      help='Directory containing data (overrides config)')
    parser.add_argument('--temperature', type=str,
                      help='Temperature to use for training (overrides config)')
    parser.add_argument('--domain-ids', type=str, nargs='+',
                      help='List of domain IDs to train on (if not specified, all available domains will be used)')

    # Model arguments
    parser.add_argument('--model', type=str, choices=['protflex_cnn', 'dilated_resnet3d', 'multipath_rmsf_net'],
                      help='Model architecture to use (overrides config)')
    parser.add_argument('--checkpoint', type=str,
                      help='Path to checkpoint to resume training from')

    # Training arguments
    parser.add_argument('--batch-size', type=int,
                      help='Batch size for training (overrides config)')
    parser.add_argument('--epochs', type=int,
                      help='Number of epochs to train (overrides config)')
    parser.add_argument('--learning-rate', type=float,
                      help='Learning rate (overrides config)')
    parser.add_argument('--loss-function', type=str, 
                      choices=['mse', 'mae', 'rmsf', 'weighted_rmsf', 'elastic_rmsf'],
                      help='Loss function to use (overrides config)')
    
    # Output arguments
    parser.add_argument('--output-dir', type=str,
                      help='Directory to save results (overrides config)')
    parser.add_argument('--no-eval', action='store_true',
                      help='Skip evaluation on test set after training')
    parser.add_argument('--cpu', action='store_true',
                      help='Force using CPU even if GPU is available')

    return parser

def run(args: argparse.Namespace, config: Dict[str, Any]) -> int:
    """
    Run the train command.

    Args:
        args: Command line arguments
        config: Configuration dictionary

    Returns:
        Exit code
    """
    # Override config with command-line arguments
    if args.data_dir:
        config['input']['data_dir'] = args.data_dir
    if args.temperature:
        config['input']['temperature'] = args.temperature
    if args.domain_ids:
        config['input']['domain_ids'] = args.domain_ids
    if args.model:
        config['model']['architecture'] = args.model
    if args.batch_size:
        config['training']['batch_size'] = args.batch_size
    if args.epochs:
        config['training']['num_epochs'] = args.epochs
    if args.learning_rate:
        config['training']['learning_rate'] = args.learning_rate
    if args.output_dir:
        config['output']['base_dir'] = args.output_dir
    if args.loss_function:
        config['training']['loss_function'] = args.loss_function

    # Set up directories
    data_dir = config['input']['data_dir']
    voxel_dir = os.path.join(data_dir, config['input']['voxel_dir'])
    rmsf_dir = os.path.join(data_dir, config['input']['rmsf_dir'])
    output_dir = config['output']['base_dir']
    model_dir = os.path.join(output_dir, config['output']['model_dir'])
    
    # Create output directories
    ensure_dir(output_dir)
    ensure_dir(model_dir)
    
    # Set up device (CPU/GPU)
    device = torch.device("cuda:0" if torch.cuda.is_available() and not args.cpu else "cpu")
    logger.info(f"Using device: {device}")
    
    # Set random seed for reproducibility
    seed = config['training']['random_seed']
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
    
    # Create data loaders
    logger.info("Creating data loaders...")
    try:
        train_loader, val_loader, test_loader = create_data_loaders(
            voxel_dir=voxel_dir,
            rmsf_dir=rmsf_dir,
            temperature=config['input']['temperature'],
            domain_ids=config['input']['domain_ids'],
            use_metadata=config['input']['use_metadata'],
            metadata_fields=config['input']['metadata_fields'],
            batch_size=config['training']['batch_size'],
            train_split=config['training']['train_split'],
            val_split=config['training']['val_split'],
            test_split=config['training']['test_split'],
            random_seed=config['training']['random_seed'],
            use_augmentation=config['training'].get('use_augmentation', True),
            augmentation_params=config['training'].get('augmentation_params', None)
        )
        
        # Get RMSF scaler for later de-normalization
        rmsf_scaler = getattr(train_loader.dataset.dataset, 'rmsf_scaler', None)
        
    except Exception as e:
        logger.error(f"Error creating data loaders: {e}")
        return 1
    
    # Create model
    logger.info(f"Creating model: {config['model']['architecture']}")
    try:
        # Get model parameters from config
        model_params = config['model'].copy()
        model_params.pop('architecture', None)  # Remove architecture key
        
        # Create model instance
        model = cnn_models.create_model(config['model']['architecture'], model_params)
        model.to(device)
        
        # Load checkpoint if specified
        if args.checkpoint:
            logger.info(f"Loading checkpoint from {args.checkpoint}")
            checkpoint = torch.load(args.checkpoint, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
        
        # Save model summary
        summary_file = os.path.join(model_dir, "model_summary.txt")
        save_model_summary(model, summary_file)
        
    except Exception as e:
        logger.error(f"Error creating model: {e}")
        return 1
    
    # Create optimizer
    logger.info("Creating optimizer and scheduler...")
    try:
        optimizer = create_optimizer(
            model=model,
            optimizer_name=config['training'].get('optimizer', 'adam'),
            learning_rate=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay']
        )
        
        scheduler = create_scheduler(
            optimizer=optimizer,
            scheduler_name=config['training']['lr_scheduler'],
            scheduler_params=config['training'].get('lr_scheduler_params', {})
        )
        
        # Create loss function
        loss_fn = create_loss_function(
            config['training'].get('loss_function', 'mse')
        )
        
    except Exception as e:
        logger.error(f"Error creating optimizer, scheduler, or loss function: {e}")
        return 1
    
    # Create trainer
    logger.info("Setting up trainer...")
    try:
        trainer = RMSFTrainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            test_loader=None if args.no_eval else test_loader,
            optimizer=optimizer,
            scheduler=scheduler,
            loss_fn=loss_fn,
            device=device,
            output_dir=model_dir,
            rmsf_scaler=rmsf_scaler
        )
        
    except Exception as e:
        logger.error(f"Error creating trainer: {e}")
        return 1
    
    # Train the model
    logger.info("Starting training...")
    try:
        results = trainer.train(
            num_epochs=config['training']['num_epochs'],
            early_stopping_patience=config['training']['early_stopping_patience']
        )
        
        # Save training metadata
        metadata = {
            "model_architecture": config['model']['architecture'],
            "temperature": config['input']['temperature'],
            "training_time": results['training_time'],
            "best_val_loss": results['best_val_loss'],
            "final_metrics": results['final_metrics'],
            "training_params": {
                "batch_size": config['training']['batch_size'],
                "learning_rate": config['training']['learning_rate'],
                "optimizer": config['training'].get('optimizer', 'adam'),
                "scheduler": config['training']['lr_scheduler'],
                "loss_function": config['training'].get('loss_function', 'mse'),
                "epochs": config['training']['num_epochs'],
                "early_stopping_patience": config['training']['early_stopping_patience']
            },
            "model_params": model_params,
            "rmsf_stats": train_loader.dataset.dataset.get_rmsf_stats() if hasattr(train_loader.dataset.dataset, 'get_rmsf_stats') else {}
        }
        
        metadata_file = os.path.join(model_dir, "training_metadata.json")
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
            
        logger.info(f"Training completed. Results saved to {model_dir}")
        
    except Exception as e:
        logger.error(f"Error during training: {e}")
        return 1
    
    return 0
===== FILE: cli/commands/evaluate.py =====
"""
Evaluation command for ProtFlex CLI.

This module provides the command to evaluate a protein flexibility prediction model.
"""

import os
import argparse
import logging
import json
from typing import Dict, Any, List

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from protflex.models import cnn_models
from protflex.data.data_loader import create_data_loaders
from protflex.utils.visualization import (
    plot_predictions,
    plot_error_distribution,
    plot_residue_analysis
)

logger = logging.getLogger(__name__)

def add_subparser(subparsers: argparse._SubParsersAction) -> argparse.ArgumentParser:
    """
    Add a subparser for the evaluate command.

    Args:
        subparsers: Subparsers object

    Returns:
        Parser for the evaluate command
    """
    parser = subparsers.add_parser(
        'evaluate',
        help='Evaluate a trained protein flexibility prediction model',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Input arguments
    parser.add_argument('--model-path', type=str, required=True,
                      help='Path to trained model checkpoint')
    parser.add_argument('--data-dir', type=str,
                      help='Directory containing data (overrides config)')
    parser.add_argument('--temperature', type=str,
                      help='Temperature to use for evaluation (overrides config)')
    parser.add_argument('--batch-size', type=int, default=32,
                      help='Batch size for evaluation')

    # Output arguments
    parser.add_argument('--output-dir', type=str,
                      help='Directory to save evaluation results (overrides config)')
    parser.add_argument('--cpu', action='store_true',
                      help='Force using CPU even if GPU is available')

    return parser

def run(args: argparse.Namespace, config: Dict[str, Any]) -> int:
    """
    Run the evaluate command.

    Args:
        args: Command line arguments
        config: Configuration dictionary

    Returns:
        Exit code
    """
    # Override config with command-line arguments
    if args.data_dir:
        config['input']['data_dir'] = args.data_dir
    if args.temperature:
        config['input']['temperature'] = args.temperature
    if args.output_dir:
        config['output']['base_dir'] = args.output_dir

    # Set up directories
    data_dir = config['input']['data_dir']
    voxel_dir = os.path.join(data_dir, config['input']['voxel_dir'])
    rmsf_dir = os.path.join(data_dir, config['input']['rmsf_dir'])
    output_dir = config['output']['base_dir']
    results_dir = os.path.join(output_dir, config['output']['results_dir'])
    eval_dir = os.path.join(results_dir, 'evaluation')

    # Create output directories
    os.makedirs(eval_dir, exist_ok=True)

    # Load model checkpoint
    logger.info(f"Loading model from {args.model_path}")

    # Determine device (CPU/GPU)
    device = torch.device("cuda:0" if torch.cuda.is_available() and not args.cpu else "cpu")
    logger.info(f"Using device: {device}")

    try:
        checkpoint = torch.load(args.model_path, map_location=device)
        model_name = checkpoint.get('model_name', config['model']['architecture'])
        model_params = config['model'].copy()
        model_params.pop('architecture', None)

        # Create model
        model = cnn_models.create_model(model_name, model_params)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        model.eval()

        # Get temperature from checkpoint or config
        temperature = checkpoint.get('temperature', config['input']['temperature'])

        logger.info(f"Model loaded successfully. Using temperature: {temperature}")
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        return 1

    # Create data loader for evaluation
    logger.info("Creating evaluation data loader...")
    _, _, test_loader = create_data_loaders(
        voxel_dir=voxel_dir,
        rmsf_dir=rmsf_dir,
        temperature=temperature,
        domain_ids=config['input']['domain_ids'],
        use_metadata=config['input']['use_metadata'],
        metadata_fields=config['input']['metadata_fields'],
        batch_size=args.batch_size,
        train_split=0.0,
        val_split=0.0,
        test_split=1.0,  # Use all data for testing
        random_seed=config['training']['random_seed'],
        use_augmentation=False
    )

    # Get RMSF scaler from dataset
    rmsf_scaler = getattr(test_loader.dataset.dataset, 'rmsf_scaler', None)

    # Evaluate the model
    logger.info("Evaluating model...")
    all_targets = []
    all_preds = []
    all_residue_types = []

    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            # Move data to device
            if isinstance(data, tuple):
                voxel_data, metadata = data
                voxel_data = voxel_data.to(device)
                metadata = metadata.to(device)
                data = (voxel_data, metadata)
            else:
                data = data.to(device)

            # Forward pass
            if isinstance(data, tuple):
                output = model(data[0], data[1])
            else:
                output = model(data)

            # Collect predictions and targets
            targets_np = target.numpy()
            preds_np = output.cpu().numpy()

            # Denormalize if needed
            if rmsf_scaler is not None:
                targets_np = rmsf_scaler.inverse_transform(targets_np)
                preds_np = rmsf_scaler.inverse_transform(preds_np)

            all_targets.append(targets_np)
            all_preds.append(preds_np)

            # Collect residue types if available
            # This is a simplification - adapt based on your dataset
            if hasattr(test_loader.dataset.dataset, 'samples'):
                batch_indices = list(range(
                    batch_idx * test_loader.batch_size,
                    min((batch_idx + 1) * test_loader.batch_size, len(test_loader.dataset))
                ))
                for idx in batch_indices:
                    sample = test_loader.dataset.dataset.samples[idx]
                    if 'metadata' in sample and 'resname' in sample['metadata']:
                        all_residue_types.append(sample['metadata']['resname'])

    # Combine results
    all_targets = np.vstack(all_targets).flatten()
    all_preds = np.vstack(all_preds).flatten()

    # Calculate metrics
    mse = mean_squared_error(all_targets, all_preds)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(all_targets, all_preds)
    r2 = r2_score(all_targets, all_preds)

    # Log results
    logger.info(f"Evaluation metrics:")
    logger.info(f"  MSE: {mse:.4f}")
    logger.info(f"  RMSE: {rmse:.4f}")
    logger.info(f"  MAE: {mae:.4f}")
    logger.info(f"  R²: {r2:.4f}")

    # Save metrics to file
    metrics = {
        'mse': float(mse),
        'rmse': float(rmse),
        'mae': float(mae),
        'r2': float(r2),
        'num_samples': int(len(all_targets)),
        'temperature': temperature
    }

    metrics_file = os.path.join(eval_dir, 'metrics.json')
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    logger.info(f"Saved metrics to {metrics_file}")

    # Create visualizations
    logger.info("Creating visualizations...")

    # Predictions vs. Actual
    pred_plot_file = os.path.join(eval_dir, 'predictions.png')
    plot_predictions(
        predictions=all_preds,
        targets=all_targets,
        output_path=pred_plot_file,
        max_points=config['visualization'].get('max_scatter_points', 1000)
    )
    logger.info(f"Saved predictions plot to {pred_plot_file}")

    # Error distribution
    error_plot_file = os.path.join(eval_dir, 'error_distribution.png')
    plot_error_distribution(
        predictions=all_preds,
        targets=all_targets,
        output_path=error_plot_file
    )
    logger.info(f"Saved error distribution plot to {error_plot_file}")

    # Residue type analysis
    if all_residue_types:
        residue_plot_file = os.path.join(eval_dir, 'residue_analysis.png')
        plot_residue_analysis(
            predictions=all_preds,
            targets=all_targets,
            residue_types=all_residue_types,
            output_path=residue_plot_file
        )
        logger.info(f"Saved residue type analysis plot to {residue_plot_file}")

    logger.info("Evaluation completed successfully")
    return 0

===== FILE: cli/commands/__init__.py =====
"""
Command module for ProtFlex CLI.

This module provides commands for the ProtFlex command-line interface.
"""

from . import train, predict, evaluate
===== FILE: cli/__init__.py =====
"""
Command-line interface for ProtFlex.

This module provides the main entry point for the ProtFlex CLI.
"""

from .cli import main
===== FILE: data/data_loader.py =====
"""
Data loading and preprocessing utilities for ProtFlex.

This module contains functions for loading and preprocessing voxelized protein data
and corresponding RMSF values.
"""

import os
import h5py
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional, Union, Any
import logging
import glob
import random
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from collections import defaultdict

logger = logging.getLogger(__name__)

class RMSFDataset(Dataset):
    """Dataset for protein voxel grids and RMSF values."""
    
    def __init__(
        self, 
        voxel_dir: str,
        rmsf_dir: str,
        temperature: Union[int, str] = 320,
        domain_ids: Optional[List[str]] = None,
        use_metadata: bool = True,
        metadata_fields: Optional[List[str]] = None,
        transform: Optional[callable] = None,
        normalize_rmsf: bool = True,
        cache_size: int = 1000  # Maximum number of samples to cache in memory
    ):
        """
        Args:
            voxel_dir: Directory containing voxelized protein data
            rmsf_dir: Directory containing RMSF CSV files
            temperature: Temperature value (320, 348, 379, 413, 450 or "average")
            domain_ids: List of domain IDs to include (None = all domains)
            use_metadata: Whether to include metadata features
            metadata_fields: List of metadata fields to include
            transform: Optional transform to apply to the voxel data
            normalize_rmsf: Whether to normalize RMSF values
            cache_size: Maximum number of samples to cache in memory
        """
        self.voxel_dir = voxel_dir
        self.rmsf_dir = rmsf_dir
        self.temperature = str(temperature)
        self.use_metadata = use_metadata
        self.metadata_fields = metadata_fields or []
        self.transform = transform
        self.normalize_rmsf = normalize_rmsf
        self.cache_size = cache_size
        
        # Read all available domains and RMSF data
        self.domain_data = self._load_domain_data(domain_ids)
        
        # Build an index of all samples
        self.samples = self._build_sample_index()
        
        # Initialize data cache
        self.data_cache = {}
        
        # Initialize RMSF normalization
        self.rmsf_scaler = None
        if normalize_rmsf:
            self._setup_rmsf_normalization()
        
        logger.info(f"Initialized dataset with {len(self.samples)} samples " 
                   f"from {len(self.domain_data)} domains")
    
    def _load_domain_data(self, domain_ids: Optional[List[str]]) -> Dict[str, Dict]:
        """
        Load RMSF data for all domains and match with available voxel files.
        
        Args:
            domain_ids: List of domain IDs to include (None = all domains)
            
        Returns:
            Dictionary mapping domain IDs to their data
        """
        # Determine the temperature directory
        if self.temperature == "average":
            temp_dir = "average"
        else:
            temp_dir = self.temperature
        
        # Find all RMSF CSV files for the specified temperature
        rmsf_pattern = os.path.join(self.rmsf_dir, temp_dir, "*.csv")
        rmsf_files = glob.glob(rmsf_pattern)
        
        if not rmsf_files:
            raise ValueError(f"No RMSF files found in {os.path.join(self.rmsf_dir, temp_dir)}")
        
        # Extract domain IDs from file names
        if domain_ids is None:
            if self.temperature == "average":
                all_domains = [os.path.basename(f).split("_total_average_rmsf.csv")[0] for f in rmsf_files]
            else:
                all_domains = [os.path.basename(f).split(f"_temperature_{self.temperature}")[0] for f in rmsf_files]
        else:
            all_domains = domain_ids
        
        # Filter out domains that don't have both RMSF data and voxel files
        domain_data = {}
        
        for domain_id in all_domains:
            # Check for RMSF file
            if self.temperature == "average":
                rmsf_file = os.path.join(self.rmsf_dir, temp_dir, f"{domain_id}_total_average_rmsf.csv")
            else:
                rmsf_file = os.path.join(self.rmsf_dir, temp_dir, f"{domain_id}_temperature_{self.temperature}_average_rmsf.csv")
            
            # Check for voxel directory
            voxel_domain_dir = os.path.join(self.voxel_dir, domain_id, temp_dir)
            
            if os.path.exists(rmsf_file) and os.path.exists(voxel_domain_dir):
                # Load RMSF data
                try:
                    rmsf_df = pd.read_csv(rmsf_file)
                    
                    # Determine the RMSF column name
                    if self.temperature == "average":
                        rmsf_col = "average_rmsf"
                    else:
                        rmsf_col = f"rmsf_{self.temperature}"
                    
                    # Check if the RMSF column exists
                    if rmsf_col not in rmsf_df.columns:
                        logger.warning(f"RMSF column '{rmsf_col}' not found in {rmsf_file}")
                        continue
                    
                    # Find voxel files for this domain
                    voxel_files = glob.glob(os.path.join(voxel_domain_dir, "*.hdf5"))
                    
                    # Only include domains with both RMSF data and voxel files
                    if voxel_files:
                        domain_data[domain_id] = {
                            "rmsf_file": rmsf_file,
                            "rmsf_df": rmsf_df,
                            "rmsf_col": rmsf_col,
                            "voxel_dir": voxel_domain_dir,
                            "voxel_files": voxel_files
                        }
                        logger.debug(f"Added domain {domain_id} with {len(rmsf_df)} residues")
                    else:
                        logger.warning(f"No voxel files found for domain {domain_id} in {voxel_domain_dir}")
                except Exception as e:
                    logger.error(f"Error loading data for domain {domain_id}: {e}")
            else:
                logger.debug(f"Skipping domain {domain_id}: RMSF file or voxel directory missing")
        
        return domain_data
    
    def _build_sample_index(self) -> List[Dict]:
        """
        Build an index of all samples in the dataset.
        
        Returns:
            List of sample dictionaries containing domain_id, resid, etc.
        """
        samples = []
        
        for domain_id, data in self.domain_data.items():
            rmsf_df = data["rmsf_df"]
            rmsf_col = data["rmsf_col"]
            
            for _, row in rmsf_df.iterrows():
                # Extract necessary information
                resid = row["resid"]
                
                # Find the corresponding voxel file
                voxel_file = self._find_voxel_file_for_residue(domain_id, resid)
                
                if voxel_file:
                    # Create a sample entry
                    sample = {
                        "domain_id": domain_id,
                        "resid": resid,
                        "rmsf_value": row[rmsf_col],
                        "voxel_file": voxel_file
                    }
                    
                    # Add metadata if requested
                    if self.use_metadata and self.metadata_fields:
                        sample["metadata"] = {
                            field: row[field] if field in row else None
                            for field in self.metadata_fields
                        }
                    
                    samples.append(sample)
        
        return samples
    def _find_voxel_file_for_residue(self, domain_id: str, resid: int) -> Optional[str]:
        """
        Find the voxel file corresponding to a residue.
        
        This method searches for voxel files that match the given residue ID,
        using multiple matching strategies.
        
        Args:
            domain_id: Domain identifier
            resid: Residue identifier
            
        Returns:
            Path to the voxel file or None if not found
        """
        voxel_domain_dir = self.domain_data[domain_id]["voxel_dir"]
        voxel_files = self.domain_data[domain_id]["voxel_files"]
        
        # First, check if we have a direct frame file for this residue
        # Try multiple patterns:
        # Pattern 1: domain_tempXXX_frameX_resid_clean_CNOCBCA.hdf5
        # Pattern 2: domain_pdb_clean_CNOCBCA.hdf5 (for PDB files without temperature)
        # Pattern 3: domain_resid_tempXXX_frameX_clean_CNOCBCA.hdf5
        
        # Strategy 1: Look for explicit residue IDs in filenames
        matching_files = []
        for voxel_file in voxel_files:
            file_name = os.path.basename(voxel_file)
            
            # Check for residue ID in filename
            if f"_res{resid}_" in file_name or f"_residue{resid}_" in file_name:
                matching_files.append(voxel_file)
                continue
            
            # Check in file name parts
            parts = file_name.split('_')
            for i, part in enumerate(parts):
                if part.isdigit() and int(part) == resid:
                    # Check if previous part indicates this is a residue ID
                    if i > 0 and (parts[i-1].lower() == "res" or parts[i-1].lower() == "residue"):
                        matching_files.append(voxel_file)
                        break
        
        # If we found matching files, return the first one
        if matching_files:
            return matching_files[0]
        
        # Strategy 2: Check HDF5 metadata for residue information
        for voxel_file in voxel_files:
            try:
                with h5py.File(voxel_file, 'r') as f:
                    # Check metadata attributes
                    if "metadata" in f and hasattr(f["metadata"], "attrs"):
                        metadata = f["metadata"]
                        if "resid" in metadata.attrs and metadata.attrs["resid"] == resid:
                            return voxel_file
                    
                    # Check top-level attributes
                    if "resid" in f.attrs and f.attrs["resid"] == resid:
                        return voxel_file
                    
                    # Check for inputs dataset with attributes
                    if "inputs" in f and hasattr(f["inputs"], "attrs"):
                        inputs = f["inputs"]
                        if "resid" in inputs.attrs and inputs.attrs["resid"] == resid:
                            return voxel_file
                    
                    # Check if this file contains multiple residues
                    if "residues" in f and str(resid) in f["residues"]:
                        return voxel_file
            except Exception as e:
                logger.warning(f"Error checking HDF5 file {voxel_file}: {e}")
        
        # Strategy 3: Check temperature subdirectory
        temp_dir = self.temperature
        temp_subdir = os.path.join(voxel_domain_dir, temp_dir)
        if os.path.exists(temp_subdir):
            # Look for files in the temperature subdirectory
            temp_files = [f for f in os.listdir(temp_subdir) if f.endswith('.hdf5')]
            for temp_file in temp_files:
                file_path = os.path.join(temp_subdir, temp_file)
                if f"_res{resid}_" in temp_file or f"_residue{resid}_" in temp_file:
                    return file_path
        
        # Log warning and return None if no match found
        logger.warning(f"Could not find voxel file for domain {domain_id}, residue {resid}")
        return None
    
    def _setup_rmsf_normalization(self) -> None:
        """Set up normalization for RMSF values."""
        # Collect all RMSF values
        all_rmsf = [sample["rmsf_value"] for sample in self.samples]
        
        if not all_rmsf:
            logger.warning("No RMSF values found for normalization, using identity scaling")
            # Create a dummy scaler that does nothing
            self.rmsf_scaler = StandardScaler()
            self.rmsf_scaler.mean_ = np.array([0.0])
            self.rmsf_scaler.var_ = np.array([1.0])
            self.rmsf_scaler.scale_ = np.array([1.0])
            return
        
        # Create and fit a scaler
        self.rmsf_scaler = StandardScaler()
        self.rmsf_scaler.fit(np.array(all_rmsf).reshape(-1, 1))
        
        logger.info(f"RMSF normalization: mean={self.rmsf_scaler.mean_[0]:.4f}, "
                f"std={np.sqrt(self.rmsf_scaler.var_[0]):.4f}")
    
    # Add this import at the top:
    # import collections

    def _load_voxel_data(self, voxel_file: str) -> torch.Tensor:
        """
        Load voxel data from an HDF5 file with efficient caching.
        
        Args:
            voxel_file: Path to the voxel HDF5 file
            
        Returns:
            Tensor containing voxel data with shape [channels, 21, 21, 21]
        """
        # First check if this sample is cached
        if voxel_file in self.data_cache:
            # Move to end (most recently used)
            value = self.data_cache.pop(voxel_file)
            self.data_cache[voxel_file] = value
            return value["voxel_data"]
        
        # If not cached, load from disk
        try:
            with h5py.File(voxel_file, 'r') as f:
                # Extract the data based on the Aposteriori HDF5 format
                if "inputs" in f:
                    # Modern aposteriori format with "inputs" dataset
                    voxel_data = f["inputs"][:]
                    
                    # Reshape if needed
                    if len(voxel_data.shape) == 4:  # [batch, channels, height, width]
                        voxel_data = voxel_data[0]  # Take the first batch item
                    
                    # Convert to torch tensor
                    voxel_tensor = torch.from_numpy(voxel_data).float()
                else:
                    # Try other locations or formats
                    if "voxel_data" in f:
                        voxel_data = f["voxel_data"][:]
                        voxel_tensor = torch.from_numpy(voxel_data).float()
                    else:
                        # Legacy format - fallback to zeros
                        logger.warning(f"No recognized data format in {voxel_file}, using zeros")
                        voxel_tensor = torch.zeros((5, 21, 21, 21), dtype=torch.float32)
            
            # Apply any transformations
            if self.transform:
                voxel_tensor = self.transform(voxel_tensor)
            
            # Cache the tensor with LRU behavior
            if len(self.data_cache) >= self.cache_size:
                # Remove oldest item (first key in OrderedDict)
                self.data_cache.popitem(last=False)
            
            # Add new item to cache
            self.data_cache[voxel_file] = {"voxel_data": voxel_tensor}
            
            return voxel_tensor
        
        except Exception as e:
            logger.error(f"Error loading voxel file {voxel_file}: {e}")
            # Return zeros as a fallback
            return torch.zeros((5, 21, 21, 21), dtype=torch.float32)
    
    def _process_metadata(self, sample: Dict) -> torch.Tensor:
        """
        Process metadata for a sample.
        
        Args:
            sample: Sample dictionary
            
        Returns:
            Tensor containing metadata features
        """
        metadata_tensors = []
        
        if "metadata" in sample:
            metadata = sample["metadata"]
            
            # Process each metadata field
            for field in self.metadata_fields:
                if field in metadata and metadata[field] is not None:
                    # Handle different types of metadata
                    if field == "resname_encoded":
                        # One-hot encoded residue name (20 values)
                        if isinstance(metadata[field], int):
                            # Already encoded as an integer
                            one_hot = torch.zeros(20, dtype=torch.float32)
                            if 0 <= metadata[field] < 20:
                                one_hot[metadata[field]] = 1.0
                            metadata_tensors.append(one_hot)
                        else:
                            # Need to encode the residue name
                            # This is a placeholder - implement proper encoding
                            placeholder = torch.zeros(20, dtype=torch.float32)
                            metadata_tensors.append(placeholder)
                    
                    elif field == "secondary_structure_encoded":
                        # One-hot encoded secondary structure (3 values: helix, sheet, loop)
                        if isinstance(metadata[field], int):
                            one_hot = torch.zeros(3, dtype=torch.float32)
                            if 0 <= metadata[field] < 3:
                                one_hot[metadata[field]] = 1.0
                            metadata_tensors.append(one_hot)
                        else:
                            placeholder = torch.zeros(3, dtype=torch.float32)
                            metadata_tensors.append(placeholder)
                    
                    elif field == "core_exterior_encoded":
                        # One-hot encoded core/exterior (2 values)
                        if isinstance(metadata[field], int):
                            one_hot = torch.zeros(2, dtype=torch.float32)
                            if 0 <= metadata[field] < 2:
                                one_hot[metadata[field]] = 1.0
                            metadata_tensors.append(one_hot)
                        else:
                            placeholder = torch.zeros(2, dtype=torch.float32)
                            metadata_tensors.append(placeholder)
                    
                    else:
                        # Scalar values like normalized_resid, relative_accessibility
                        value = float(metadata[field])
                        metadata_tensors.append(torch.tensor([value], dtype=torch.float32))
        
        # Concatenate all metadata tensors
        if metadata_tensors:
            return torch.cat(metadata_tensors)
        else:
            # Return an empty tensor if no metadata was processed
            return torch.tensor([], dtype=torch.float32)
    
    def __len__(self) -> int:
        """Get the number of samples in the dataset."""
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get a sample from the dataset.
        
        Args:
            idx: Index of the sample
            
        Returns:
            Tuple containing the input tensor (voxel data with optional metadata)
            and the target tensor (RMSF value)
        """
        sample = self.samples[idx]
        
        # Load voxel data
        voxel_data = self._load_voxel_data(sample["voxel_file"])
        
        # Process metadata if needed
        if self.use_metadata and self.metadata_fields:
            metadata = self._process_metadata(sample)
            input_data = (voxel_data, metadata)
        else:
            input_data = voxel_data
        
        # Process target RMSF value
        rmsf_value = sample["rmsf_value"]
        if self.normalize_rmsf and self.rmsf_scaler is not None:
            rmsf_value = self.rmsf_scaler.transform([[rmsf_value]])[0, 0]
        
        target = torch.tensor([rmsf_value], dtype=torch.float32)
        
        return input_data, target

    def get_rmsf_stats(self) -> Dict[str, float]:
        """
        Get statistics for RMSF values.
        
        Returns:
            Dictionary of RMSF statistics
        """
        if self.rmsf_scaler is not None:
            return {
                "mean": float(self.rmsf_scaler.mean_[0]),
                "std": float(np.sqrt(self.rmsf_scaler.var_[0])),
                "min": float(min(sample["rmsf_value"] for sample in self.samples)),
                "max": float(max(sample["rmsf_value"] for sample in self.samples))
            }
        else:
            rmsf_values = [sample["rmsf_value"] for sample in self.samples]
            return {
                "mean": float(np.mean(rmsf_values)),
                "std": float(np.std(rmsf_values)),
                "min": float(min(rmsf_values)),
                "max": float(max(rmsf_values))
            }

    def get_residue_type_distribution(self) -> Dict[str, int]:
        """
        Get the distribution of residue types in the dataset.
        
        Returns:
            Dictionary mapping residue names to counts
        """
        residue_counts = defaultdict(int)
        
        for domain_id, data in self.domain_data.items():
            rmsf_df = data["rmsf_df"]
            if "resname" in rmsf_df.columns:
                for resname in rmsf_df["resname"]:
                    residue_counts[resname] += 1
        
        return dict(residue_counts)


# Augmentation functions
class RandomRotation3D:
    """Apply random 3D rotation to voxel grids."""
    
    def __init__(self, prob: float = 0.5, angles: List[int] = [90, 180, 270]):
        """
        Args:
            prob: Probability of applying the rotation
            angles: Possible rotation angles in degrees
        """
        self.prob = prob
        self.angles = angles
    
    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        if random.random() < self.prob:
            # Choose a random axis and angle
            axis = random.randint(0, 2)  # 0=x, 1=y, 2=z
            angle = random.choice(self.angles)
            
            # Convert to numpy for rotation
            x_np = x.numpy()
            
            # Rotate along the chosen axis
            k = angle // 90  # Number of 90-degree rotations
            
            if axis == 0:
                x_np = np.rot90(x_np, k=k, axes=(2, 3))
            elif axis == 1:
                x_np = np.rot90(x_np, k=k, axes=(1, 3))
            else:
                x_np = np.rot90(x_np, k=k, axes=(1, 2))
            
            return torch.from_numpy(x_np)
        
        return x

class RandomFlip3D:
    """Apply random 3D flipping to voxel grids."""
    
    def __init__(self, prob: float = 0.3):
        """
        Args:
            prob: Probability of applying the flip
        """
        self.prob = prob
    
    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        if random.random() < self.prob:
            # Choose a random axis
            axis = random.randint(1, 3)  # 1=x, 2=y, 3=z
            
            # Flip along the chosen axis
            if axis == 1:
                return x.flip(1)
            elif axis == 2:
                return x.flip(2)
            else:
                return x.flip(3)
        
        return x

class AddGaussianNoise:
    """Add Gaussian noise to voxel grids."""
    
    def __init__(self, prob: float = 0.2, scale: float = 0.05):
        """
        Args:
            prob: Probability of adding noise
            scale: Scale of the Gaussian noise
        """
        self.prob = prob
        self.scale = scale
    
    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        if random.random() < self.prob:
            noise = torch.randn_like(x) * self.scale
            return x + noise
        
        return x

class ComposeTransforms:
    """Compose multiple transforms together."""
    
    def __init__(self, transforms: List[callable]):
        """
        Args:
            transforms: List of transform callables
        """
        self.transforms = transforms
    
    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        for transform in self.transforms:
            x = transform(x)
        return x

def create_data_loaders(
    voxel_dir: str,
    rmsf_dir: str,
    temperature: Union[int, str] = 320,
    domain_ids: Optional[List[str]] = None,
    use_metadata: bool = True,
    metadata_fields: Optional[List[str]] = None,
    batch_size: int = 32,
    train_split: float = 0.7,
    val_split: float = 0.15,
    test_split: float = 0.15,
    random_seed: int = 42,
    num_workers: int = 4,
    use_augmentation: bool = True,
    augmentation_params: Optional[Dict[str, Any]] = None
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    Create train, validation, and test data loaders.
    
    Args:
        voxel_dir: Directory containing voxelized protein data
        rmsf_dir: Directory containing RMSF CSV files
        temperature: Temperature value or "average"
        domain_ids: List of domain IDs to include (None = all domains)
        use_metadata: Whether to include metadata features
        metadata_fields: List of metadata fields to include
        batch_size: Batch size for data loaders
        train_split: Fraction of data to use for training
        val_split: Fraction of data to use for validation
        test_split: Fraction of data to use for testing
        random_seed: Random seed for reproducibility
        num_workers: Number of worker threads for data loading
        use_augmentation: Whether to use data augmentation
        augmentation_params: Parameters for data augmentation
        
    Returns:
        Tuple of (train_loader, val_loader, test_loader)
    """
    # Validate split proportions
    assert 0.0 <= train_split <= 1.0, "Train split must be between 0 and 1"
    assert 0.0 <= val_split <= 1.0, "Validation split must be between 0 and 1"
    assert 0.0 <= test_split <= 1.0, "Test split must be between 0 and 1"
    assert abs(train_split + val_split + test_split - 1.0) < 1e-6, "Split proportions must sum to 1"
    
    # Set random seed for reproducibility
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)
    random.seed(random_seed)
    
    # Create transforms
    train_transform = None
    if use_augmentation:
        params = augmentation_params or {}
        train_transform = ComposeTransforms([
            RandomRotation3D(
                prob=params.get("rotation_prob", 0.5),
                angles=params.get("rotation_angles", [90, 180, 270])
            ),
            RandomFlip3D(prob=params.get("flip_prob", 0.3)),
            AddGaussianNoise(
                prob=params.get("noise_prob", 0.2),
                scale=params.get("noise_scale", 0.05)
            )
        ])
    
    # Create the full dataset
    full_dataset = RMSFDataset(
        voxel_dir=voxel_dir,
        rmsf_dir=rmsf_dir,
        temperature=temperature,
        domain_ids=domain_ids,
        use_metadata=use_metadata,
        metadata_fields=metadata_fields,
        transform=None,  # We'll apply transforms in the subset datasets
        normalize_rmsf=True
    )
    
    # Get RMSF statistics
    rmsf_stats = full_dataset.get_rmsf_stats()
    logger.info(f"RMSF statistics: mean={rmsf_stats['mean']:.4f}, std={rmsf_stats['std']:.4f}, "
              f"min={rmsf_stats['min']:.4f}, max={rmsf_stats['max']:.4f}")
    
    # Split the dataset
    dataset_size = len(full_dataset)
    indices = list(range(dataset_size))
    np.random.shuffle(indices)
    
    train_end = int(train_split * dataset_size)
    val_end = train_end + int(val_split * dataset_size)
    
    train_indices = indices[:train_end]
    val_indices = indices[train_end:val_end]
    test_indices = indices[val_end:]
    
    logger.info(f"Data split: train={len(train_indices)}, val={len(val_indices)}, test={len(test_indices)}")
    
    # Create subset datasets
    class TransformSubset(torch.utils.data.Subset):
        def __init__(self, dataset, indices, transform=None):
            super().__init__(dataset, indices)
            self.transform = transform
        
        def __getitem__(self, idx):
            input_data, target = super().__getitem__(idx)
            
            if self.transform is not None:
                if isinstance(input_data, tuple):
                    voxel_data, metadata = input_data
                    voxel_data = self.transform(voxel_data)
                    input_data = (voxel_data, metadata)
                else:
                    input_data = self.transform(input_data)
            
            return input_data, target
    
    train_dataset = TransformSubset(full_dataset, train_indices, train_transform)
    val_dataset = TransformSubset(full_dataset, val_indices, None)
    test_dataset = TransformSubset(full_dataset, test_indices, None)
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader, test_loader
===== FILE: data/datasets.py =====
"""
Dataset management for ProtFlex.

This module provides functions for loading, processing, and managing protein structure datasets.
"""

import os
import glob
import logging
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from tqdm import tqdm

logger = logging.getLogger(__name__)

def load_dataset(data_dir: str, domain_ids: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    Load a dataset from the specified directory.
    
    Args:
        data_dir: Path to the data directory
        domain_ids: Optional list of domain IDs to load (if None, load all domains)
    
    Returns:
        Dictionary containing loaded dataset information
    """
    logger.info(f"Loading dataset from {data_dir}")
    
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"Data directory not found: {data_dir}")
    
    # Find all domain directories
    domain_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]
    
    if domain_ids is not None:
        domain_dirs = [d for d in domain_dirs if d in domain_ids]
    
    if not domain_dirs:
        logger.warning(f"No domain directories found in {data_dir}")
        return {}
    
    # Load domain data
    dataset = {}
    
    for domain_id in tqdm(domain_dirs, desc="Loading domains"):
        try:
            domain_data = load_domain_data(os.path.join(data_dir, domain_id), domain_id)
            if domain_data:
                dataset[domain_id] = domain_data
        except Exception as e:
            logger.error(f"Error loading domain {domain_id}: {e}")
    
    logger.info(f"Loaded {len(dataset)} domains")
    return dataset

def load_domain_data(domain_dir: str, domain_id: str) -> Dict[str, Any]:
    """
    Load data for a single protein domain.
    
    Args:
        domain_dir: Path to the domain directory
        domain_id: Domain identifier
    
    Returns:
        Dictionary containing domain data
    """
    # Validate domain directory
    if not os.path.exists(domain_dir):
        logger.warning(f"Domain directory not found: {domain_dir}")
        return {}
    
    # Find voxel and RMSF directories/files
    voxel_files = glob.glob(os.path.join(domain_dir, "**", "*.hdf5"), recursive=True)
    rmsf_files = glob.glob(os.path.join(domain_dir, "**", "*.csv"), recursive=True)
    
    if not voxel_files:
        logger.warning(f"No voxel files found for domain {domain_id}")
        return {}
    
    if not rmsf_files:
        logger.warning(f"No RMSF files found for domain {domain_id}")
        return {}
    
    # Create domain data dictionary
    domain_data = {
        "domain_id": domain_id,
        "voxel_files": voxel_files,
        "rmsf_files": rmsf_files
    }
    
    # Try to load RMSF data
    try:
        rmsf_df = pd.read_csv(rmsf_files[0])
        
        # Validate RMSF data
        if "resid" not in rmsf_df.columns:
            logger.warning(f"RMSF file for domain {domain_id} is missing 'resid' column")
            return {}
            
        # Find RMSF column
        rmsf_cols = [col for col in rmsf_df.columns if "rmsf" in col.lower()]
        if not rmsf_cols:
            logger.warning(f"No RMSF column found in data for domain {domain_id}")
            return {}
            
        domain_data["rmsf_data"] = rmsf_df
        domain_data["rmsf_column"] = rmsf_cols[0]  # Use first RMSF column
    except Exception as e:
        logger.error(f"Error loading RMSF data for domain {domain_id}: {e}")
        return {}
    
    return domain_data

def filter_dataset(dataset: Dict[str, Any], min_residues: int = 10) -> Dict[str, Any]:
    """
    Filter dataset based on certain criteria.
    
    Args:
        dataset: Dataset dictionary
        min_residues: Minimum number of residues required for a domain
    
    Returns:
        Filtered dataset
    """
    filtered_dataset = {}
    
    for domain_id, domain_data in dataset.items():
        # Check for required data
        if "rmsf_data" not in domain_data:
            logger.warning(f"Domain {domain_id} missing RMSF data, skipping")
            continue
        
        # Check minimum residue count
        if len(domain_data["rmsf_data"]) < min_residues:
            logger.warning(f"Domain {domain_id} has fewer than {min_residues} residues, skipping")
            continue
        
        # Add to filtered dataset
        filtered_dataset[domain_id] = domain_data
    
    logger.info(f"Filtered dataset from {len(dataset)} to {len(filtered_dataset)} domains")
    return filtered_dataset

def split_dataset(
    dataset: Dict[str, Any],
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    random_seed: int = 42
) -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    """
    Split dataset into training, validation, and test sets.
    
    Args:
        dataset: Dataset dictionary
        train_ratio: Proportion for training set
        val_ratio: Proportion for validation set
        test_ratio: Proportion for test set
        random_seed: Random seed for reproducibility
    
    Returns:
        Tuple of (train_dataset, val_dataset, test_dataset)
    """
    import random
    random.seed(random_seed)
    
    # Check that ratios sum to 1
    total_ratio = train_ratio + val_ratio + test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        logger.warning(f"Split ratios do not sum to 1: {total_ratio}")
        # Normalize ratios
        train_ratio = train_ratio / total_ratio
        val_ratio = val_ratio / total_ratio
        test_ratio = test_ratio / total_ratio
    
    # Get list of domain IDs
    domain_ids = list(dataset.keys())
    random.shuffle(domain_ids)
    
    # Calculate split indices
    n_domains = len(domain_ids)
    n_train = int(n_domains * train_ratio)
    n_val = int(n_domains * val_ratio)
    
    # Split domain IDs
    train_ids = domain_ids[:n_train]
    val_ids = domain_ids[n_train:n_train + n_val]
    test_ids = domain_ids[n_train + n_val:]
    
    # Create split datasets
    train_dataset = {domain_id: dataset[domain_id] for domain_id in train_ids}
    val_dataset = {domain_id: dataset[domain_id] for domain_id in val_ids}
    test_dataset = {domain_id: dataset[domain_id] for domain_id in test_ids}
    
    logger.info(f"Split dataset into {len(train_dataset)} training, {len(val_dataset)} validation, and {len(test_dataset)} test domains")
    
    return train_dataset, val_dataset, test_dataset
===== FILE: data/__init__.py =====
"""
Data module for ProtFlex.

This module handles loading and preprocessing of protein voxel data and RMSF values.
"""

from .data_loader import (
    RMSFDataset,
    RandomRotation3D,
    RandomFlip3D,
    AddGaussianNoise,
    ComposeTransforms,
    create_data_loaders
)

from .dataset import convert_to_pytorch_dataset
from .datasets import load_dataset, filter_dataset, split_dataset
===== FILE: data/dataset.py =====
"""
Datasets management for ProtFlex.

This module provides utilities for loading, filtering, and splitting datasets
for protein flexibility prediction.
"""

import os
import glob
import numpy as np
import pandas as pd
import h5py
from typing import Dict, List, Tuple, Optional, Union, Any
import logging
import random

logger = logging.getLogger(__name__)

def load_dataset(
    data_dir: str,
    voxel_dir: str,
    rmsf_dir: str,
    temperature: Union[int, str] = 320,
    domain_ids: Optional[List[str]] = None
) -> Dict[str, Dict[str, Any]]:
    """
    Load dataset information for training.
    
    Args:
        data_dir: Base data directory
        voxel_dir: Subdirectory containing voxelized data
        rmsf_dir: Subdirectory containing RMSF data
        temperature: Temperature value or "average"
        domain_ids: List of domain IDs to include (None = all domains)
        
    Returns:
        Dictionary mapping domain IDs to their data
    """
    # Convert temperature to string for path construction
    temp_str = str(temperature)
    
    # Set up paths
    voxel_base_dir = os.path.join(data_dir, voxel_dir)
    rmsf_base_dir = os.path.join(data_dir, rmsf_dir)
    
    # Determine temperature directories
    if temp_str == "average":
        rmsf_temp_dir = os.path.join(rmsf_base_dir, "average")
    else:
        rmsf_temp_dir = os.path.join(rmsf_base_dir, temp_str)
    
    # Validate paths
    if not os.path.exists(rmsf_temp_dir):
        logger.error(f"RMSF directory does not exist: {rmsf_temp_dir}")
        return {}
    
    # Get domain IDs based on availability of RMSF files
    available_domains = []
    
    if domain_ids is None:
        # Find all domains with RMSF data for the specified temperature
        rmsf_files = glob.glob(os.path.join(rmsf_temp_dir, "*.csv"))
        
        for rmsf_file in rmsf_files:
            file_name = os.path.basename(rmsf_file)
            
            if temp_str == "average":
                if "_total_average_rmsf.csv" in file_name:
                    domain_id = file_name.replace("_total_average_rmsf.csv", "")
                    available_domains.append(domain_id)
            else:
                if f"_temperature_{temp_str}" in file_name:
                    domain_id = file_name.split(f"_temperature_{temp_str}")[0]
                    available_domains.append(domain_id)
                elif f"_temp{temp_str}" in file_name:
                    domain_id = file_name.split(f"_temp{temp_str}")[0]
                    available_domains.append(domain_id)
    else:
        # Use the provided domain IDs
        available_domains = domain_ids
    
    # Load data for each domain
    dataset = {}
    
    for domain_id in available_domains:
        # Find RMSF file
        if temp_str == "average":
            rmsf_file = os.path.join(rmsf_temp_dir, f"{domain_id}_total_average_rmsf.csv")
            if not os.path.exists(rmsf_file):
                # Try alternative naming
                rmsf_file = os.path.join(rmsf_temp_dir, f"{domain_id}_average_rmsf.csv")
        else:
            rmsf_file = os.path.join(rmsf_temp_dir, f"{domain_id}_temperature_{temp_str}_average_rmsf.csv")
            if not os.path.exists(rmsf_file):
                # Try alternative naming
                rmsf_file = os.path.join(rmsf_temp_dir, f"{domain_id}_temp{temp_str}_average_rmsf.csv")
                if not os.path.exists(rmsf_file):
                    # Try another alternative naming
                    rmsf_file = os.path.join(rmsf_temp_dir, f"{domain_id}_rmsf_{temp_str}.csv")
        
        # Skip if RMSF file not found
        if not os.path.exists(rmsf_file):
            logger.warning(f"RMSF file not found for domain {domain_id}")
            continue
        
        # Find voxel directory
        voxel_domain_dir = os.path.join(voxel_base_dir, domain_id)
        
        # Check if domain directory exists
        if not os.path.exists(voxel_domain_dir):
            logger.warning(f"Voxel directory not found for domain {domain_id}")
            continue
        
        # Find voxel files based on temperature
        voxel_files = []
        
        # Check if temperature-specific directory exists
        temp_dir = os.path.join(voxel_domain_dir, temp_str)
        if os.path.exists(temp_dir):
            # Temperature-specific directory exists
            voxel_files = glob.glob(os.path.join(temp_dir, "*.hdf5"))
        else:
            # Try alternative directories
            pdb_dir = os.path.join(voxel_domain_dir, "pdb")
            if os.path.exists(pdb_dir):
                voxel_files = glob.glob(os.path.join(pdb_dir, "*.hdf5"))
            else:
                # Look directly in domain directory
                voxel_files = glob.glob(os.path.join(voxel_domain_dir, "*.hdf5"))
        
        # Skip if no voxel files found
        if not voxel_files:
            logger.warning(f"No voxel files found for domain {domain_id}")
            continue
        
        # Load RMSF data
        try:
            rmsf_df = pd.read_csv(rmsf_file)
            
            # Determine RMSF column name
            rmsf_columns = [col for col in rmsf_df.columns if 'rmsf' in col.lower()]
            if not rmsf_columns:
                logger.warning(f"No RMSF column found in {rmsf_file}")
                continue
            
            # Use the first matching column as default
            rmsf_col = rmsf_columns[0]
            
            # Try to find a more specific column if multiple exist
            if temp_str == "average":
                for col in rmsf_columns:
                    if 'average' in col.lower():
                        rmsf_col = col
                        break
            else:
                for col in rmsf_columns:
                    if temp_str in col:
                        rmsf_col = col
                        break
            
            # Store data
            dataset[domain_id] = {
                "rmsf_file": rmsf_file,
                "rmsf_df": rmsf_df,
                "rmsf_col": rmsf_col,
                "voxel_dir": voxel_domain_dir,
                "voxel_files": voxel_files,
                "temperature": temp_str
            }
            
            logger.info(f"Loaded data for domain {domain_id} with {len(rmsf_df)} residues and {len(voxel_files)} voxel files")
            
        except Exception as e:
            logger.error(f"Error loading data for domain {domain_id}: {e}")
    
    logger.info(f"Loaded dataset with {len(dataset)} domains")
    return dataset

def filter_dataset(
    dataset: Dict[str, Dict[str, Any]],
    min_residues: int = 10,
    max_residues: Optional[int] = None,
    min_voxel_files: int = 5,
    max_voxel_files: Optional[int] = None
) -> Dict[str, Dict[str, Any]]:
    """
    Filter dataset based on criteria.
    
    Args:
        dataset: Dictionary mapping domain IDs to their data
        min_residues: Minimum number of residues required
        max_residues: Maximum number of residues allowed (None = no limit)
        min_voxel_files: Minimum number of voxel files required
        max_voxel_files: Maximum number of voxel files allowed (None = no limit)
        
    Returns:
        Filtered dataset
    """
    filtered_dataset = {}
    
    for domain_id, data in dataset.items():
        # Check number of residues
        num_residues = len(data["rmsf_df"])
        if num_residues < min_residues:
            logger.debug(f"Filtering out domain {domain_id}: too few residues ({num_residues} < {min_residues})")
            continue
        
        if max_residues is not None and num_residues > max_residues:
            logger.debug(f"Filtering out domain {domain_id}: too many residues ({num_residues} > {max_residues})")
            continue
        
        # Check number of voxel files
        num_voxel_files = len(data["voxel_files"])
        if num_voxel_files < min_voxel_files:
            logger.debug(f"Filtering out domain {domain_id}: too few voxel files ({num_voxel_files} < {min_voxel_files})")
            continue
        
        if max_voxel_files is not None and num_voxel_files > max_voxel_files:
            logger.debug(f"Filtering out domain {domain_id}: too many voxel files ({num_voxel_files} > {max_voxel_files})")
            continue
        
        # Include domain in filtered dataset
        filtered_dataset[domain_id] = data
    
    logger.info(f"Filtered dataset from {len(dataset)} to {len(filtered_dataset)} domains")
    return filtered_dataset

def match_residues_to_voxels(
    dataset: Dict[str, Dict[str, Any]]
) -> Dict[str, Dict[str, Any]]:
    """
    Match residues in RMSF files to voxel files.
    
    Args:
        dataset: Dictionary mapping domain IDs to their data
        
    Returns:
        Dataset with matched residues and voxel files
    """
    matched_dataset = {}
    
    for domain_id, data in dataset.items():
        rmsf_df = data["rmsf_df"]
        voxel_files = data["voxel_files"]
        rmsf_col = data["rmsf_col"]
        
        # Create a mapping of residue IDs to voxel files
        residue_to_voxel = {}
        
        # Extract residue IDs from RMSF dataframe
        residue_ids = []
        if "resid" in rmsf_df.columns:
            residue_ids = rmsf_df["resid"].tolist()
        elif "residue_id" in rmsf_df.columns:
            residue_ids = rmsf_df["residue_id"].tolist()
        else:
            # Try to extract from index if it's numeric
            try:
                residue_ids = [int(idx) for idx in rmsf_df.index]
            except:
                logger.warning(f"Could not determine residue IDs for domain {domain_id}")
                continue
        
        # Match residues to voxel files
        for residue_id in residue_ids:
            # Look for voxel files that match this residue
            matching_files = []
            
            for voxel_file in voxel_files:
                file_name = os.path.basename(voxel_file)
                
                # Try to extract residue ID from filename using different patterns
                try:
                    # Option 1: Residue ID after "res" or "residue"
                    if f"res{residue_id}_" in file_name or f"residue{residue_id}_" in file_name:
                        matching_files.append(voxel_file)
                        continue
                    
                    # Option 2: Residue ID as part of split filename
                    parts = file_name.split('_')
                    for i, part in enumerate(parts):
                        if part.isdigit() and int(part) == residue_id:
                            # Check if previous part is "res" or "residue"
                            if i > 0 and (parts[i-1].lower() == "res" or parts[i-1].lower() == "residue"):
                                matching_files.append(voxel_file)
                                break
                    
                    # Option 3: Check HDF5 file metadata
                    try:
                        with h5py.File(voxel_file, 'r') as f:
                            if "metadata" in f and "resid" in f["metadata"].attrs:
                                if f["metadata"].attrs["resid"] == residue_id:
                                    matching_files.append(voxel_file)
                            elif "resid" in f.attrs:
                                if f.attrs["resid"] == residue_id:
                                    matching_files.append(voxel_file)
                    except:
                        pass  # Silently continue if HDF5 metadata check fails
                    
                except:
                    pass  # Silently continue if parsing fails
            
            # Store the matching files
            if matching_files:
                residue_to_voxel[residue_id] = matching_files
        
        # Create matched samples for this domain
        matched_samples = []
        
        # For each row in the RMSF dataframe
        for _, row in rmsf_df.iterrows():
            # Get residue ID
            residue_id = None
            if "resid" in row:
                residue_id = row["resid"]
            elif "residue_id" in row:
                residue_id = row["residue_id"]
            else:
                try:
                    residue_id = int(row.name)
                except:
                    continue
            
            # Get RMSF value
            if rmsf_col in row:
                rmsf_value = row[rmsf_col]
            else:
                logger.warning(f"RMSF column {rmsf_col} not found in row for domain {domain_id}, residue {residue_id}")
                continue
            
            # Get matching voxel files
            if residue_id in residue_to_voxel:
                voxel_files_for_residue = residue_to_voxel[residue_id]
                
                # Create a sample for each matching voxel file
                for voxel_file in voxel_files_for_residue:
                    sample = {
                        "domain_id": domain_id,
                        "residue_id": residue_id,
                        "rmsf_value": rmsf_value,
                        "voxel_file": voxel_file
                    }
                    
                    # Add metadata if available
                    metadata = {}
                    
                    if "resname" in row:
                        metadata["resname"] = row["resname"]
                    
                    if "secondary_structure" in row:
                        metadata["secondary_structure"] = row["secondary_structure"]
                    
                    if "accessibility" in row:
                        metadata["accessibility"] = row["accessibility"]
                    
                    if metadata:
                        sample["metadata"] = metadata
                    
                    matched_samples.append(sample)
        
        # Store matched samples for this domain
        if matched_samples:
            matched_dataset[domain_id] = {
                "samples": matched_samples,
                "rmsf_col": rmsf_col,
                "temperature": data["temperature"]
            }
            logger.info(f"Matched {len(matched_samples)} samples for domain {domain_id}")
    
    logger.info(f"Created matched dataset with {len(matched_dataset)} domains")
    return matched_dataset

def split_dataset(
    dataset: Dict[str, Dict[str, Any]],
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    random_seed: int = 42
) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """
    Split dataset into train, validation, and test sets.
    
    Args:
        dataset: Dictionary mapping domain IDs to their data
        train_ratio: Ratio of data to use for training
        val_ratio: Ratio of data to use for validation
        test_ratio: Ratio of data to use for testing
        random_seed: Random seed for reproducibility
        
    Returns:
        Tuple of (train_dataset, val_dataset, test_dataset)
    """
    # Validate split ratios
    if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
        logger.warning(f"Split ratios do not sum to 1: {train_ratio} + {val_ratio} + {test_ratio} = {train_ratio + val_ratio + test_ratio}")
        # Normalize ratios
        total = train_ratio + val_ratio + test_ratio
        train_ratio /= total
        val_ratio /= total
        test_ratio /= total
    
    # Set random seed for reproducibility
    random.seed(random_seed)
    
    # Get all domain IDs
    domain_ids = list(dataset.keys())
    random.shuffle(domain_ids)
    
    # Calculate split indices
    train_end = int(len(domain_ids) * train_ratio)
    val_end = train_end + int(len(domain_ids) * val_ratio)
    
    # Split domain IDs
    train_domains = domain_ids[:train_end]
    val_domains = domain_ids[train_end:val_end]
    test_domains = domain_ids[val_end:]
    
    # Create split datasets
    train_dataset = {domain_id: dataset[domain_id] for domain_id in train_domains}
    val_dataset = {domain_id: dataset[domain_id] for domain_id in val_domains}
    test_dataset = {domain_id: dataset[domain_id] for domain_id in test_domains}
    
    logger.info(f"Split dataset into {len(train_dataset)} train, {len(val_dataset)} validation, and {len(test_dataset)} test domains")
    return train_dataset, val_dataset, test_dataset

def create_residue_level_dataset(
    matched_dataset: Dict[str, Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Create a residue-level dataset from matched domain data.
    
    Args:
        matched_dataset: Matched domain data from match_residues_to_voxels
        
    Returns:
        List of samples for all residues
    """
    all_samples = []
    
    for domain_id, data in matched_dataset.items():
        all_samples.extend(data["samples"])
    
    logger.info(f"Created residue-level dataset with {len(all_samples)} samples")
    return all_samples
===== FILE: models/cnn_models.py =====
"""
Advanced 3D CNN architectures for protein flexibility prediction.

This module contains implementations of various 3D CNN architectures designed
to predict RMSF (Root Mean Square Fluctuation) values from voxelized protein
structures. The networks are specifically designed to preserve spatial relationships
among backbone atoms (C, N, O, CA, CB) which are crucial for understanding
protein dynamics.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple, Optional, Union

class ResidualBlock3D(nn.Module):
    """A 3D residual block with optional dilation and bottle-necking.
    
    This block implements the core residual connection pattern where the input
    is added to the output of a series of convolutional operations. It supports
    dilated convolutions to increase the receptive field without increasing
    parameter count.
    """
    def __init__(
        self, 
        in_channels: int, 
        out_channels: int, 
        kernel_size: int = 3, 
        stride: int = 1, 
        dilation: int = 1, 
        bottleneck: bool = False, 
        bottleneck_factor: int = 4
    ):
        """
        Args:
            in_channels: Number of input channels
            out_channels: Number of output channels
            kernel_size: Size of the convolutional kernel
            stride: Stride of the convolution
            dilation: Dilation rate of the convolution
            bottleneck: Whether to use bottleneck architecture
            bottleneck_factor: Bottleneck reduction factor
        """
        super().__init__()
        
        self.bottleneck = bottleneck
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        # Calculate padding to maintain spatial dimensions
        padding = dilation * (kernel_size - 1) // 2
        
        if bottleneck:
            # Bottleneck architecture - reduce channels, then 3x3, then expand channels
            bottleneck_channels = out_channels // bottleneck_factor
            self.conv1 = nn.Conv3d(in_channels, bottleneck_channels, kernel_size=1)
            self.bn1 = nn.BatchNorm3d(bottleneck_channels)
            self.conv2 = nn.Conv3d(bottleneck_channels, bottleneck_channels, 
                                kernel_size=kernel_size, stride=stride, 
                                padding=padding, dilation=dilation)
            self.bn2 = nn.BatchNorm3d(bottleneck_channels)
            self.conv3 = nn.Conv3d(bottleneck_channels, out_channels, kernel_size=1)
            self.bn3 = nn.BatchNorm3d(out_channels)
        else:
            # Standard ResNet block - two 3x3 convolutions
            self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size,
                                 stride=stride, padding=padding, dilation=dilation)
            self.bn1 = nn.BatchNorm3d(out_channels)
            self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size,
                                 padding=padding, dilation=dilation)
            self.bn2 = nn.BatchNorm3d(out_channels)
        
        # Shortcut connection
        if in_channels != out_channels or stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm3d(out_channels)
            )
        else:
            self.shortcut = nn.Identity()
            
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = self.shortcut(x)
        
        if self.bottleneck:
            out = F.relu(self.bn1(self.conv1(x)))
            out = F.relu(self.bn2(self.conv2(out)))
            out = self.bn3(self.conv3(out))
        else:
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.bn2(self.conv2(out))
        
        out += residual
        return F.relu(out)

class MultiscaleLayer3D(nn.Module):
    """A multiscale feature extraction layer using parallel convolutions.
    
    This layer performs parallel convolutions with different kernel sizes and 
    dilations to capture features at multiple scales, then concatenates 
    the results.
    """
    def __init__(
        self, 
        in_channels: int, 
        out_channels: int, 
        kernel_sizes: List[int] = [3, 5],
        dilations: List[int] = [1, 2, 3]
    ):
        """
        Args:
            in_channels: Number of input channels
            out_channels: Number of output channels per parallel path
            kernel_sizes: List of kernel sizes for parallel paths
            dilations: List of dilation rates for parallel paths
        """
        super().__init__()
        
        self.paths = nn.ModuleList()
        total_paths = len(kernel_sizes) * len(dilations)
        path_channels = out_channels // total_paths
        
        # Ensure we use all output channels by assigning the remainder to the first path
        remainder = out_channels - (path_channels * total_paths)
        
        path_count = 0
        for k in kernel_sizes:
            for d in dilations:
                # Determine output channels for this path
                if path_count == 0:
                    path_out_channels = path_channels + remainder
                else:
                    path_out_channels = path_channels
                
                # Calculate padding to maintain spatial dimensions
                padding = d * (k - 1) // 2
                
                # Create the convolutional path
                path = nn.Sequential(
                    nn.Conv3d(in_channels, path_out_channels, kernel_size=k, 
                              padding=padding, dilation=d),
                    nn.BatchNorm3d(path_out_channels),
                    nn.ReLU(inplace=True)
                )
                self.paths.append(path)
                path_count += 1
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        outputs = [path(x) for path in self.paths]
        return torch.cat(outputs, dim=1)

class ProtFlexCNN(nn.Module):
    """3D CNN model for protein flexibility prediction.
    
    This model architecture is specifically designed for predicting residue flexibility
    from voxelized protein structures. It employs 3D convolutions with residual
    connections and multiscale feature extraction to capture both local atomic
    arrangements and global structural contexts.
    """
    def __init__(
        self, 
        input_channels: int = 5,  # C, N, O, CA, CB channels
        channel_growth_rate: float = 1.5,
        num_residual_blocks: int = 4,
        use_multiscale: bool = True,
        use_bottleneck: bool = True,
        dropout_rate: float = 0.2,
        include_metadata: bool = False,
        metadata_features: int = 0
    ):
        """
        Args:
            input_channels: Number of input channels (default 5 for CNOCBCA encoding)
            channel_growth_rate: Factor by which channel count increases in deeper layers
            num_residual_blocks: Number of residual blocks in the network
            use_multiscale: Whether to use multiscale feature extraction
            use_bottleneck: Whether to use bottleneck architecture in residual blocks
            dropout_rate: Dropout probability in fully connected layers
            include_metadata: Whether to include additional metadata features
            metadata_features: Number of metadata features to include
        """
        super().__init__()
        
        self.include_metadata = include_metadata
        self.metadata_features = metadata_features
        
        # Initial 3D convolution for feature extraction
        base_channels = 32
        self.initial_conv = nn.Sequential(
            nn.Conv3d(input_channels, base_channels, kernel_size=5, padding=2),
            nn.BatchNorm3d(base_channels),
            nn.ReLU(inplace=True)
        )
        
        # Feature extraction using residual blocks and multiscale convolutions
        self.feature_extractor = nn.ModuleList()
        in_channels = base_channels
        
        for i in range(num_residual_blocks):
            # Calculate output channels with growth rate
            out_channels = int(in_channels * channel_growth_rate)
            
            # Decide on stride for downsampling
            # Downsample after the 1st and 3rd residual block
            stride = 2 if i in [1, 3] else 1
            
            # Add a multiscale feature extraction layer
            if use_multiscale and i < num_residual_blocks - 1:
                self.feature_extractor.append(
                    MultiscaleLayer3D(in_channels, out_channels, 
                                     kernel_sizes=[3, 5], 
                                     dilations=[1, 2, 3])
                )
                in_channels = out_channels
            
            # Add residual block
            self.feature_extractor.append(
                ResidualBlock3D(in_channels, out_channels, 
                               kernel_size=3, stride=stride,
                               dilation=1, bottleneck=use_bottleneck)
            )
            in_channels = out_channels
        
        # Global average pooling to reduce spatial dimensions
        self.global_avg_pool = nn.AdaptiveAvgPool3d(1)
        
        # Fully connected layers for regression
        fc_input_size = in_channels
        if include_metadata:
            fc_input_size += metadata_features
            
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(fc_input_size, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(64, 1)  # Output a single RMSF value
        )
        
    def forward(self, x: torch.Tensor, metadata: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Forward pass through the CNN.
        
        Args:
            x: Input tensor of shape [batch_size, channels, depth, height, width]
            metadata: Optional tensor of additional features
            
        Returns:
            Tensor of RMSF predictions with shape [batch_size, 1]
        """
        # Initial convolution
        x = self.initial_conv(x)
        
        # Feature extraction blocks
        for layer in self.feature_extractor:
            x = layer(x)
        
        # Global average pooling
        x = self.global_avg_pool(x)
        
        # Combine with metadata if provided
        if self.include_metadata and metadata is not None:
            x = x.view(x.size(0), -1)  # Flatten
            x = torch.cat([x, metadata], dim=1)
        
        # Fully connected layers for regression
        x = self.fc_layers(x)
        
        return x

class DilatedResNet3D(nn.Module):
    """A more advanced 3D ResNet with dilated convolutions specialized for protein structures.
    
    This model focuses heavily on capturing long-range interactions through extensive
    use of dilated convolutions while maintaining high-resolution feature maps.
    """
    def __init__(
        self,
        input_channels: int = 5,
        init_features: int = 32,
        block_config: List[int] = [2, 2, 2, 2],
        dilations: List[int] = [1, 2, 4, 8],
        include_metadata: bool = False,
        metadata_features: int = 0,
        dropout_rate: float = 0.3
    ):
        """
        Args:
            input_channels: Number of input channels
            init_features: Initial number of feature maps
            block_config: Number of residual blocks in each stage
            dilations: Dilation rates for each stage
            include_metadata: Whether to include additional metadata features
            metadata_features: Number of metadata features
            dropout_rate: Dropout probability in fully connected layers
        """
        super().__init__()
        
        self.include_metadata = include_metadata
        self.metadata_features = metadata_features
        
        # Initial convolution
        self.conv1 = nn.Sequential(
            nn.Conv3d(input_channels, init_features, kernel_size=7, stride=1, padding=3),
            nn.BatchNorm3d(init_features),
            nn.ReLU(inplace=True)
        )
        
        # Create residual stages
        self.stages = nn.ModuleList()
        in_channels = init_features
        
        for i, (num_blocks, dilation) in enumerate(zip(block_config, dilations)):
            # Double the number of features at each stage
            out_channels = in_channels * 2 if i > 0 else in_channels
            
            # First block of each stage may include strided convolution for downsampling
            # We only downsample in the first two stages to preserve spatial information
            stride = 2 if i < 2 else 1
            
            # Create a sequential container for this stage
            stage = nn.Sequential()
            
            # Add the first block with potential stride
            stage.add_module(
                f"block_{i}_0",
                ResidualBlock3D(in_channels, out_channels, stride=stride, 
                               dilation=dilation, bottleneck=True)
            )
            
            # Add the remaining blocks
            for j in range(1, num_blocks):
                stage.add_module(
                    f"block_{i}_{j}",
                    ResidualBlock3D(out_channels, out_channels, 
                                   dilation=dilation, bottleneck=True)
                )
            
            self.stages.append(stage)
            in_channels = out_channels
        
        # Global pooling
        self.global_pool = nn.AdaptiveAvgPool3d(1)
        
        # Fully connected layers for regression
        fc_input_size = in_channels
        if include_metadata:
            fc_input_size += metadata_features
            
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(fc_input_size, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 1)  # Output a single RMSF value
        )
    
    def forward(self, x: torch.Tensor, metadata: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Forward pass through the network."""
        # Initial convolution
        x = self.conv1(x)
        
        # Process through all stages
        for stage in self.stages:
            x = stage(x)
        
        # Global pooling
        x = self.global_pool(x)
        
        # Combine with metadata if provided
        if self.include_metadata and metadata is not None:
            x = x.view(x.size(0), -1)  # Flatten
            x = torch.cat([x, metadata], dim=1)
        
        # Regression head
        x = self.fc_layers(x)
        
        return x

class MultipathRMSFNet(nn.Module):
    """A CNN with multiple parallel paths to capture different aspects of protein structure.
    
    This network uses separate pathways to process different atom types and structural scales,
    then combines them for the final prediction. This design is inspired by how different
    structural elements (backbone geometry, side chain orientation, etc.) contribute to
    protein flexibility.
    """
    def __init__(
        self,
        input_channels: int = 5,
        base_filters: int = 32,
        include_metadata: bool = False,
        metadata_features: int = 0,
        dropout_rate: float = 0.3
    ):
        """
        Args:
            input_channels: Number of input channels
            base_filters: Base number of filters for convolutional layers
            include_metadata: Whether to include additional metadata features
            metadata_features: Number of metadata features
            dropout_rate: Dropout probability in fully connected layers
        """
        super().__init__()
        
        self.include_metadata = include_metadata
        self.metadata_features = metadata_features
        
        # We'll create three parallel paths with different kernel and dilation configurations
        # Path 1: Focus on local features with small kernels
        self.local_path = nn.Sequential(
            nn.Conv3d(input_channels, base_filters, kernel_size=3, padding=1),
            nn.BatchNorm3d(base_filters),
            nn.ReLU(inplace=True),
            ResidualBlock3D(base_filters, base_filters * 2, kernel_size=3, stride=2),
            ResidualBlock3D(base_filters * 2, base_filters * 2),
            ResidualBlock3D(base_filters * 2, base_filters * 4, stride=2),
            ResidualBlock3D(base_filters * 4, base_filters * 4)
        )
        
        # Path 2: Focus on medium-range interactions with medium kernels and dilations
        self.medium_path = nn.Sequential(
            nn.Conv3d(input_channels, base_filters, kernel_size=5, padding=2),
            nn.BatchNorm3d(base_filters),
            nn.ReLU(inplace=True),
            ResidualBlock3D(base_filters, base_filters * 2, kernel_size=3, stride=2, dilation=2),
            ResidualBlock3D(base_filters * 2, base_filters * 2, dilation=2),
            ResidualBlock3D(base_filters * 2, base_filters * 4, stride=2),
            ResidualBlock3D(base_filters * 4, base_filters * 4, dilation=2)
        )
        
        # Path 3: Focus on long-range interactions with large dilations
        self.global_path = nn.Sequential(
            nn.Conv3d(input_channels, base_filters, kernel_size=7, padding=3),
            nn.BatchNorm3d(base_filters),
            nn.ReLU(inplace=True),
            ResidualBlock3D(base_filters, base_filters * 2, kernel_size=3, stride=2, dilation=4),
            ResidualBlock3D(base_filters * 2, base_filters * 2, dilation=4),
            ResidualBlock3D(base_filters * 2, base_filters * 4, stride=2),
            ResidualBlock3D(base_filters * 4, base_filters * 4, dilation=4)
        )
        
        # Global pooling for each path
        self.pool = nn.AdaptiveAvgPool3d(1)
        
        # Combine features from all paths
        combined_features = base_filters * 4 * 3  # 3 paths with base_filters * 4 features each
        if include_metadata:
            combined_features += metadata_features
        
        # Fully connected layers for regression
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(combined_features, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 1)  # Output a single RMSF value
        )
    
    def forward(self, x: torch.Tensor, metadata: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Forward pass through the network."""
        # Process through each path
        local_features = self.pool(self.local_path(x))
        medium_features = self.pool(self.medium_path(x))
        global_features = self.pool(self.global_path(x))
        
        # Concatenate features from all paths
        combined = torch.cat([
            local_features.view(local_features.size(0), -1),
            medium_features.view(medium_features.size(0), -1),
            global_features.view(global_features.size(0), -1)
        ], dim=1)
        
        # Combine with metadata if provided
        if self.include_metadata and metadata is not None:
            combined = torch.cat([combined, metadata], dim=1)
        
        # Final prediction
        return self.fc_layers(combined)

# Factory function to create models based on configuration
def create_model(model_name: str, model_params: Dict) -> nn.Module:
    """
    Factory function to create a model instance based on name and parameters.
    
    Args:
        model_name: Name of the model architecture to use
        model_params: Dictionary of model parameters
        
    Returns:
        Instance of the requested model
    """
    if model_name == "protflex_cnn":
        return ProtFlexCNN(**model_params)
    elif model_name == "dilated_resnet3d":
        return DilatedResNet3D(**model_params)
    elif model_name == "multipath_rmsf_net":
        return MultipathRMSFNet(**model_params)
    else:
        raise ValueError(f"Unknown model architecture: {model_name}")
===== FILE: models/loss.py =====
"""
Custom loss functions for protein flexibility prediction.

This module provides specialized loss functions for training RMSF prediction models.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional

class RMSFLoss(nn.Module):
    """
    Custom loss function for RMSF prediction that combines MSE and RMSE.

    This loss function is designed specifically for RMSF prediction, using a weighted
    combination of MSE (mean squared error) and RMSE (root mean squared error) to
    better handle the natural distribution of RMSF values.
    """

    def __init__(self, mse_weight: float = 0.7, rmse_weight: float = 0.3, eps: float = 1e-8):
        """
        Initialize the RMSF loss function.

        Args:
            mse_weight: Weight for the MSE component
            rmse_weight: Weight for the RMSE component
            eps: Small constant to avoid numerical instability
        """
        super().__init__()
        self.mse_weight = mse_weight
        self.rmse_weight = rmse_weight
        self.eps = eps

    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:
        """
        Calculate the loss.

        Args:
            y_pred: Predicted values
            y_true: True values

        Returns:
            Weighted loss value
        """
        # Mean squared error
        mse = F.mse_loss(y_pred, y_true)

        # Root mean squared error
        rmse = torch.sqrt(mse + self.eps)

        # Combined loss
        loss = self.mse_weight * mse + self.rmse_weight * rmse

        return loss

class WeightedRMSFLoss(nn.Module):
    """
    Weighted loss function that gives higher weight to high RMSF values.

    This loss function places more emphasis on accurately predicting residues with
    high flexibility (high RMSF), which are often functionally important but fewer
    in number than low-RMSF residues.
    """

    def __init__(self, threshold: float = 0.5, high_weight: float = 2.0, low_weight: float = 1.0):
        """
        Initialize the weighted RMSF loss function.

        Args:
            threshold: RMSF threshold to separate high and low values
            high_weight: Weight for high RMSF values
            low_weight: Weight for low RMSF values
        """
        super().__init__()
        self.threshold = threshold
        self.high_weight = high_weight
        self.low_weight = low_weight

    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:
        """
        Calculate the weighted loss.

        Args:
            y_pred: Predicted values
            y_true: True values

        Returns:
            Weighted loss value
        """
        # Create weight mask based on true values
        weights = torch.ones_like(y_true)
        weights[y_true > self.threshold] = self.high_weight
        weights[y_true <= self.threshold] = self.low_weight

        # Calculate squared differences
        squared_diff = (y_pred - y_true) ** 2

        # Weighted MSE
        loss = torch.mean(weights * squared_diff)

        return loss

class ElasticRMSFLoss(nn.Module):
    """
    Elastic net style loss combining L1 and L2 penalties for RMSF prediction.

    This loss function combines MSE (L2) and MAE (L1) losses, similar to an elastic
    net regularization, to benefit from both the smooth gradients of MSE and the
    robustness to outliers of MAE.
    """

    def __init__(self, l1_weight: float = 0.5, l2_weight: float = 0.5):
        """
        Initialize the elastic RMSF loss function.

        Args:
            l1_weight: Weight for the L1 (MAE) component
            l2_weight: Weight for the L2 (MSE) component
        """
        super().__init__()
        self.l1_weight = l1_weight
        self.l2_weight = l2_weight

    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:
        """
        Calculate the elastic loss.

        Args:
            y_pred: Predicted values
            y_true: True values

        Returns:
            Elastic loss value
        """
        # L1 loss (MAE)
        l1_loss = F.l1_loss(y_pred, y_true)

        # L2 loss (MSE)
        l2_loss = F.mse_loss(y_pred, y_true)

        # Combined loss
        loss = self.l1_weight * l1_loss + self.l2_weight * l2_loss

        return loss

def create_loss_function(loss_name: str, **kwargs) -> nn.Module:
    """
    Factory function to create a loss function based on name and parameters.

    Args:
        loss_name: Name of the loss function
        **kwargs: Additional parameters for the loss function

    Returns:
        Loss function instance
    """
    if loss_name == "mse":
        return nn.MSELoss()
    elif loss_name == "mae":
        return nn.L1Loss()
    elif loss_name == "rmsf":
        return RMSFLoss(**kwargs)
    elif loss_name == "weighted_rmsf":
        return WeightedRMSFLoss(**kwargs)
    elif loss_name == "elastic_rmsf":
        return ElasticRMSFLoss(**kwargs)
    else:
        raise ValueError(f"Unknown loss function: {loss_name}")

===== FILE: models/__init__.py =====
"""
Model module for ProtFlex.

This module provides neural network models for protein flexibility prediction.
"""

from .cnn_models import create_model, ProtFlexCNN, DilatedResNet3D, MultipathRMSFNet
from .loss import create_loss_function, RMSFLoss, WeightedRMSFLoss, ElasticRMSFLoss

===== FILE: models/layers.py =====
"""
Custom neural network layers for protein flexibility prediction.

This module provides specialized neural network layers for processing 3D protein structure data.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple, Optional, Union

class SpatialAttention3D(nn.Module):
    """
    3D Spatial Attention layer for highlighting important regions in voxel data.
    """
    
    def __init__(self, in_channels: int, kernel_size: int = 7):
        """
        Initialize the spatial attention layer.
        
        Args:
            in_channels: Number of input channels
            kernel_size: Size of the convolutional kernel
        """
        super().__init__()
        assert kernel_size % 2 == 1, "Kernel size must be odd"
        padding = kernel_size // 2
        
        self.conv = nn.Conv3d(2, 1, kernel_size=kernel_size, padding=padding)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            x: Input tensor of shape [batch_size, channels, depth, height, width]
            
        Returns:
            Tensor with same shape as input, with spatial attention applied
        """
        # Calculate max and average along channel dimension
        max_pool = torch.max(x, dim=1, keepdim=True)[0]
        avg_pool = torch.mean(x, dim=1, keepdim=True)
        
        # Concatenate the pooled features
        pooled = torch.cat([max_pool, avg_pool], dim=1)
        
        # Apply convolution and sigmoid activation
        attention = self.sigmoid(self.conv(pooled))
        
        # Apply attention to input
        return x * attention

class ChannelAttention3D(nn.Module):
    """
    Channel Attention layer for highlighting important atom channels.
    """
    
    def __init__(self, in_channels: int, reduction_ratio: int = 8):
        """
        Initialize the channel attention layer.
        
        Args:
            in_channels: Number of input channels
            reduction_ratio: Reduction ratio for the bottleneck
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.max_pool = nn.AdaptiveMaxPool3d(1)
        
        # Shared MLP for channel attention
        self.mlp = nn.Sequential(
            nn.Conv3d(in_channels, in_channels // reduction_ratio, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(in_channels // reduction_ratio, in_channels, kernel_size=1)
        )
        
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            x: Input tensor of shape [batch_size, channels, depth, height, width]
            
        Returns:
            Tensor with same shape as input, with channel attention applied
        """
        # Apply average pooling
        avg_out = self.mlp(self.avg_pool(x))
        
        # Apply max pooling
        max_out = self.mlp(self.max_pool(x))
        
        # Combine pooled features and apply sigmoid
        attention = self.sigmoid(avg_out + max_out)
        
        # Apply attention to input
        return x * attention
===== FILE: training/trainer.py =====
"""
Model training and evaluation for ProtFlex.

This module provides utilities for training and evaluating models that predict
protein flexibility (RMSF) from voxelized protein structures.
"""

import os
import time
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
import logging
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import json

from protflex.utils.visualization import (
    plot_loss_curves,
    plot_predictions,
    plot_residue_analysis,
    plot_error_distribution
)

logger = logging.getLogger(__name__)

class EarlyStopping:
    """Early stopping to prevent overfitting."""
    
    def __init__(self, patience: int = 7, min_delta: float = 0.0, mode: str = 'min'):
        """
        Args:
            patience: Number of epochs to wait for improvement
            min_delta: Minimum change to qualify as improvement
            mode: 'min' for metrics to minimize, 'max' for metrics to maximize
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, current_score: float) -> bool:
        """
        Check if training should be stopped.
        
        Args:
            current_score: Current validation score
            
        Returns:
            True if training should be stopped, False otherwise
        """
        if self.best_score is None:
            self.best_score = current_score
            return False
        
        if self.mode == 'min':
            if current_score < self.best_score - self.min_delta:
                self.best_score = current_score
                self.counter = 0
            else:
                self.counter += 1
        else:  # mode == 'max'
            if current_score > self.best_score + self.min_delta:
                self.best_score = current_score
                self.counter = 0
            else:
                self.counter += 1
        
        if self.counter >= self.patience:
            self.early_stop = True
            return True
        
        return False

class MetricTracker:
    """Track and compute metrics during training and evaluation."""
    
    def __init__(self, metrics: Optional[List[str]] = None):
        """
        Args:
            metrics: List of metric names to track
        """
        self.metrics = metrics or ["loss", "mse", "mae", "r2"]
        self.reset()
    
    def reset(self) -> None:
        """Reset all tracked metrics."""
        self.values = {metric: [] for metric in self.metrics}
        self.counts = {metric: 0 for metric in self.metrics}
        self.running_total = {metric: 0.0 for metric in self.metrics}
    
    def update(self, metric: str, value: float, batch_size: int = 1) -> None:
        """
        Update a metric.
        
        Args:
            metric: Name of the metric
            value: Metric value
            batch_size: Batch size for weighted averaging
        """
        if metric not in self.metrics:
            self.metrics.append(metric)
            self.values[metric] = []
            self.counts[metric] = 0
            self.running_total[metric] = 0.0
        
        self.values[metric].append(value)
        self.running_total[metric] += value * batch_size
        self.counts[metric] += batch_size
    
    def avg(self, metric: str) -> float:
        """
        Get the average value for a metric.
        
        Args:
            metric: Name of the metric
            
        Returns:
            Average value of the metric
        """
        if self.counts[metric] == 0:
            return 0.0
        return self.running_total[metric] / self.counts[metric]
    
    def result(self) -> Dict[str, float]:
        """
        Get the averaged results for all metrics.
        
        Returns:
            Dictionary mapping metric names to average values
        """
        return {metric: self.avg(metric) for metric in self.metrics}
    
    def history(self, metric: str) -> List[float]:
        """
        Get the history of values for a metric.
        
        Args:
            metric: Name of the metric
            
        Returns:
            List of values for the metric
        """
        return self.values.get(metric, [])

class RMSFTrainer:
    """Trainer for RMSF prediction models."""
    
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: Optional[DataLoader] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        loss_fn: Optional[Callable] = None,
        device: Optional[torch.device] = None,
        output_dir: str = "results",
        rmsf_scaler = None
    ):
        """
        Args:
            model: Neural network model
            train_loader: DataLoader for training data
            val_loader: DataLoader for validation data
            test_loader: DataLoader for test data (optional)
            optimizer: Optimizer for training
            scheduler: Learning rate scheduler
            loss_fn: Loss function
            device: Device for training
            output_dir: Directory to save results
            rmsf_scaler: Scaler used to normalize RMSF values
        """
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        
        # Set up device
        self.device = device or (torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu"))
        self.model.to(self.device)
        
        # Set up training components
        self.optimizer = optimizer or optim.Adam(model.parameters(), lr=0.001)
        self.scheduler = scheduler
        self.loss_fn = loss_fn or nn.MSELoss()
        
        # Set up tracking
        self.train_metrics = MetricTracker()
        self.val_metrics = MetricTracker()
        self.test_metrics = MetricTracker()
        
        # Set up output directory
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Save training history
        self.history = {
            "train_loss": [],
            "val_loss": [],
            "lr": []
        }
        
        # Save best model state
        self.best_val_loss = float("inf")
        self.best_model_path = os.path.join(self.output_dir, "best_model.pth")
        
        # Save predictions
        self.all_preds = []
        self.all_targets = []
        
        # Save RMSF scaler for denormalization
        self.rmsf_scaler = rmsf_scaler
        
        logger.info(f"Initialized trainer with model: {type(model).__name__}")
        logger.info(f"Training on device: {self.device}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """
        Train for one epoch.
        
        Args:
            epoch: Current epoch number
            
        Returns:
            Dictionary of training metrics
        """
        # Set model to training mode
        self.model.train()
        
        # Reset metrics
        self.train_metrics.reset()
        
        # Training loop
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")
        for i, (data, target) in enumerate(pbar):
            # Move data to device
            if isinstance(data, tuple):
                voxel_data, metadata = data
                voxel_data = voxel_data.to(self.device)
                metadata = metadata.to(self.device)
                data = (voxel_data, metadata)
            else:
                data = data.to(self.device)
            
            target = target.to(self.device)
            
            # Zero gradients
            self.optimizer.zero_grad()
            
            # Forward pass
            if isinstance(data, tuple):
                output = self.model(data[0], data[1])
            else:
                output = self.model(data)
            
            # Calculate loss
            loss = self.loss_fn(output, target)
            
            # Backward pass and update
            loss.backward()
            self.optimizer.step()
            
            # Update metrics
            batch_size = target.size(0)
            self.train_metrics.update("loss", loss.item(), batch_size)
            
            # Convert to NumPy arrays for metrics calculation
            output_np = output.detach().cpu().numpy()
            target_np = target.detach().cpu().numpy()
            
            # Calculate additional metrics
            mse = mean_squared_error(target_np, output_np)
            mae = mean_absolute_error(target_np, output_np)
            
            # R² can be undefined if all targets are identical (zero variance)
            try:
                r2 = r2_score(target_np, output_np)
            except:
                r2 = 0.0
            
            self.train_metrics.update("mse", mse, batch_size)
            self.train_metrics.update("mae", mae, batch_size)
            self.train_metrics.update("r2", r2, batch_size)
            
            # Update progress bar
            pbar.set_postfix({
                "loss": f"{self.train_metrics.avg('loss'):.4f}",
                "mse": f"{self.train_metrics.avg('mse'):.4f}",
                "mae": f"{self.train_metrics.avg('mae'):.4f}",
                "r2": f"{self.train_metrics.avg('r2'):.4f}"
            })
        
        # Log results
        metrics = self.train_metrics.result()
        logger.info(f"Epoch {epoch} - Train: loss={metrics['loss']:.4f}, mse={metrics['mse']:.4f}, "
                   f"mae={metrics['mae']:.4f}, r2={metrics['r2']:.4f}")
        
        return metrics
    
    def validate(self, epoch: int) -> Dict[str, float]:
        """
        Validate the model.
        
        Args:
            epoch: Current epoch number
            
        Returns:
            Dictionary of validation metrics
        """
        # Set model to evaluation mode
        self.model.eval()
        
        # Reset metrics
        self.val_metrics.reset()
        
        # Validation loop
        with torch.no_grad():
            for data, target in self.val_loader:
                # Move data to device
                if isinstance(data, tuple):
                    voxel_data, metadata = data
                    voxel_data = voxel_data.to(self.device)
                    metadata = metadata.to(self.device)
                    data = (voxel_data, metadata)
                else:
                    data = data.to(self.device)
                
                target = target.to(self.device)
                
                # Forward pass
                if isinstance(data, tuple):
                    output = self.model(data[0], data[1])
                else:
                    output = self.model(data)
                
                # Calculate loss
                loss = self.loss_fn(output, target)
                
                # Update metrics
                batch_size = target.size(0)
                self.val_metrics.update("loss", loss.item(), batch_size)
                
                # Convert to NumPy arrays for metrics calculation
                output_np = output.detach().cpu().numpy()
                target_np = target.detach().cpu().numpy()
                
                # Calculate additional metrics
                mse = mean_squared_error(target_np, output_np)
                mae = mean_absolute_error(target_np, output_np)
                
                try:
                    r2 = r2_score(target_np, output_np)
                except:
                    r2 = 0.0
                
                self.val_metrics.update("mse", mse, batch_size)
                self.val_metrics.update("mae", mae, batch_size)
                self.val_metrics.update("r2", r2, batch_size)
        
        # Log results
        metrics = self.val_metrics.result()
        logger.info(f"Epoch {epoch} - Validation: loss={metrics['loss']:.4f}, mse={metrics['mse']:.4f}, "
                   f"mae={metrics['mae']:.4f}, r2={metrics['r2']:.4f}")
        
        return metrics
    
    def test(self) -> Dict[str, float]:
        """
        Test the model on the test dataset.
        
        Returns:
            Dictionary of test metrics
        """
        if self.test_loader is None:
            logger.warning("No test loader provided. Skipping test.")
            return {}
        
        # Load best model if available
        if os.path.exists(self.best_model_path):
            logger.info(f"Loading best model from {self.best_model_path}")
            self.model.load_state_dict(torch.load(self.best_model_path))
        
        # Set model to evaluation mode
        self.model.eval()
        
        # Reset metrics
        self.test_metrics.reset()
        
        # Collect all predictions and targets for later analysis
        all_outputs = []
        all_targets = []
        
        # Test loop
        with torch.no_grad():
            for data, target in tqdm(self.test_loader, desc="Testing"):
                # Move data to device
                if isinstance(data, tuple):
                    voxel_data, metadata = data
                    voxel_data = voxel_data.to(self.device)
                    metadata = metadata.to(self.device)
                    data = (voxel_data, metadata)
                else:
                    data = data.to(self.device)
                
                target = target.to(self.device)
                
                # Forward pass
                if isinstance(data, tuple):
                    output = self.model(data[0], data[1])
                else:
                    output = self.model(data)
                
                # Calculate loss
                loss = self.loss_fn(output, target)
                
                # Update metrics
                batch_size = target.size(0)
                self.test_metrics.update("loss", loss.item(), batch_size)
                
                # Convert to NumPy arrays for metrics calculation
                output_np = output.detach().cpu().numpy()
                target_np = target.detach().cpu().numpy()
                
                # Collect predictions and targets
                all_outputs.append(output_np)
                all_targets.append(target_np)
                
                # Calculate additional metrics
                mse = mean_squared_error(target_np, output_np)
                mae = mean_absolute_error(target_np, output_np)
                
                try:
                    r2 = r2_score(target_np, output_np)
                except:
                    r2 = 0.0
                
                self.test_metrics.update("mse", mse, batch_size)
                self.test_metrics.update("mae", mae, batch_size)
                self.test_metrics.update("r2", r2, batch_size)
        
        # Combine all predictions and targets
        self.all_preds = np.vstack(all_outputs)
        self.all_targets = np.vstack(all_targets)
        
        # Denormalize if scaler is provided
        if self.rmsf_scaler is not None:
            self.all_preds = self.rmsf_scaler.inverse_transform(self.all_preds)
            self.all_targets = self.rmsf_scaler.inverse_transform(self.all_targets)
        
        # Recalculate metrics on the entire test set
        mse = mean_squared_error(self.all_targets, self.all_preds)
        mae = mean_absolute_error(self.all_targets, self.all_preds)
        
        try:
            r2 = r2_score(self.all_targets, self.all_preds)
        except:
            r2 = 0.0
        
        # Set the final metrics
        metrics = {
            "mse": mse,
            "mae": mae,
            "r2": r2,
            "loss": mse  # Use MSE as the loss
        }
        
        # Log results
        logger.info(f"Test Results: mse={metrics['mse']:.4f}, mae={metrics['mae']:.4f}, r2={metrics['r2']:.4f}")
        
        return metrics
    
    def train(self, num_epochs: int, early_stopping_patience: int = 10) -> Dict[str, Any]:
        """
        Train the model for a specified number of epochs.
        
        Args:
            num_epochs: Number of training epochs
            early_stopping_patience: Patience for early stopping
            
        Returns:
            Dictionary containing training history and results
        """
        # Set up early stopping
        early_stopping = EarlyStopping(patience=early_stopping_patience)
        
        # Set up timing
        start_time = time.time()
        
        # Training loop
        for epoch in range(1, num_epochs + 1):
            # Train for one epoch
            train_metrics = self.train_epoch(epoch)
            
            # Validate
            val_metrics = self.validate(epoch)
            
            # Update learning rate
            if self.scheduler is not None:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_metrics["loss"])
                else:
                    self.scheduler.step()
            
            # Get current learning rate
            current_lr = self.optimizer.param_groups[0]["lr"]
            
            # Update history
            self.history["train_loss"].append(train_metrics["loss"])
            self.history["val_loss"].append(val_metrics["loss"])
            self.history["lr"].append(current_lr)
            
            # Check for improvement
            if val_metrics["loss"] < self.best_val_loss:
                self.best_val_loss = val_metrics["loss"]
                logger.info(f"New best validation loss: {self.best_val_loss:.4f}")
                
                # Save the best model
                torch.save(self.model.state_dict(), self.best_model_path)
            
            # Early stopping
            if early_stopping(val_metrics["loss"]):
                logger.info(f"Early stopping triggered after {epoch} epochs")
                break
        
        # Calculate training time
        train_time = time.time() - start_time
        logger.info(f"Training completed in {train_time:.2f} seconds")
        
        # Test the model
        test_metrics = self.test()
        
        # Save training history
        history_path = os.path.join(self.output_dir, "training_history.json")
        with open(history_path, "w") as f:
            json.dump({
                "train_loss": self.history["train_loss"],
                "val_loss": self.history["val_loss"],
                "lr": self.history["lr"],
                "final_metrics": {
                    "train": train_metrics,
                    "val": val_metrics,
                    "test": test_metrics
                },
                "training_time": train_time
            }, f, indent=4)
        
        # Create visualization plots
        self.create_visualization_plots()
        
        # Return training results
        return {
            "history": self.history,
            "final_metrics": {
                "train": train_metrics,
                "val": val_metrics,
                "test": test_metrics
            },
            "training_time": train_time,
            "best_val_loss": self.best_val_loss
        }
    
    def create_visualization_plots(self) -> None:
        """Create and save visualization plots."""
        # Create visualizations directory
        vis_dir = os.path.join(self.output_dir, "visualizations")
        os.makedirs(vis_dir, exist_ok=True)
        
        # Plot loss curves
        loss_curve_path = os.path.join(vis_dir, "loss_curves.png")
        plot_loss_curves(
            train_loss=self.history["train_loss"],
            val_loss=self.history["val_loss"],
            lr=self.history["lr"],
            output_path=loss_curve_path
        )
        
        # Plot predictions vs targets
        if len(self.all_preds) > 0:
            predictions_path = os.path.join(vis_dir, "predictions.png")
            plot_predictions(
                predictions=self.all_preds.flatten(),
                targets=self.all_targets.flatten(),
                output_path=predictions_path,
                max_points=1000  # Limit for better visualization
            )
            
            # Plot error distribution
            error_dist_path = os.path.join(vis_dir, "error_distribution.png")
            plot_error_distribution(
                predictions=self.all_preds.flatten(),
                targets=self.all_targets.flatten(),
                output_path=error_dist_path
            )
            
            # Note: residue-type analysis would need additional data about residue types
            # This could be added if the dataset provides this information


def create_optimizer(
    model: nn.Module,
    optimizer_name: str = "adam",
    learning_rate: float = 0.001,
    weight_decay: float = 1e-5
) -> torch.optim.Optimizer:
    """
    Create an optimizer for the model.
    
    Args:
        model: Neural network model
        optimizer_name: Name of the optimizer (adam, sgd, adagrad, etc.)
        learning_rate: Learning rate
        weight_decay: Weight decay (L2 regularization)
        
    Returns:
        Optimizer instance
    """
    optimizer_name = optimizer_name.lower()
    
    if optimizer_name == "adam":
        return optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name == "sgd":
        return optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)
    elif optimizer_name == "adagrad":
        return optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name == "rmsprop":
        return optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    else:
        logger.warning(f"Unknown optimizer: {optimizer_name}. Using Adam instead.")
        return optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

def create_scheduler(
    optimizer: torch.optim.Optimizer,
    scheduler_name: str = "reduce_on_plateau",
    scheduler_params: Optional[Dict[str, Any]] = None
) -> Optional[torch.optim.lr_scheduler._LRScheduler]:
    """
    Create a learning rate scheduler.
    
    Args:
        optimizer: Optimizer instance
        scheduler_name: Name of the scheduler
        scheduler_params: Additional scheduler parameters
        
    Returns:
        Scheduler instance or None
    """
    params = scheduler_params or {}
    scheduler_name = scheduler_name.lower()
    
    if scheduler_name == "reduce_on_plateau":
        return optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode="min",
            factor=params.get("factor", 0.5),
            patience=params.get("patience", 5),
            verbose=True
        )
    elif scheduler_name == "cosine_annealing":
        return optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=params.get("t_max", 10),
            eta_min=params.get("eta_min", 0)
        )
    elif scheduler_name == "step":
        return optim.lr_scheduler.StepLR(
            optimizer,
            step_size=params.get("step_size", 30),
            gamma=params.get("gamma", 0.1)
        )
    elif scheduler_name == "exponential":
        return optim.lr_scheduler.ExponentialLR(
            optimizer,
            gamma=params.get("gamma", 0.95)
        )
    elif scheduler_name == "none" or not scheduler_name:
        return None
    else:
        logger.warning(f"Unknown scheduler: {scheduler_name}. Not using a scheduler.")
        return None
===== FILE: training/validators.py =====
"""
Validation utilities for ProtFlex.

This module provides validators for dataset integrity and model performance.
"""

import os
import logging
import numpy as np
import torch
import h5py
from typing import Dict, List, Tuple, Optional, Union, Any

logger = logging.getLogger(__name__)

class DataValidator:
    """
    Validator for checking dataset integrity.
    """
    
    def __init__(self, voxel_dir: str, rmsf_dir: str):
        """
        Initialize the validator.
        
        Args:
            voxel_dir: Directory containing voxelized data
            rmsf_dir: Directory containing RMSF data
        """
        self.voxel_dir = voxel_dir
        self.rmsf_dir = rmsf_dir
    
    def validate_domain(self, domain_id: str, temperature: str) -> Dict[str, Any]:
        """
        Validate data for a single domain.
        
        Args:
            domain_id: Domain identifier
            temperature: Temperature value
            
        Returns:
            Dictionary with validation results
        """
        logger.info(f"Validating domain {domain_id} at temperature {temperature}")
        
        results = {
            "domain_id": domain_id,
            "temperature": temperature,
            "is_valid": False,
            "voxel_dir_exists": False,
            "rmsf_file_exists": False,
            "voxel_files_count": 0,
            "residues_with_voxel_data": 0,
            "issues": []
        }
        
        # Check voxel directory
        domain_voxel_dir = os.path.join(self.voxel_dir, domain_id, str(temperature))
        results["voxel_dir_exists"] = os.path.exists(domain_voxel_dir)
        
        if not results["voxel_dir_exists"]:
            results["issues"].append(f"Voxel directory not found: {domain_voxel_dir}")
            return results
        
        # Check RMSF file
        if temperature == "average":
            rmsf_file = os.path.join(self.rmsf_dir, "average", f"{domain_id}_total_average_rmsf.csv")
        else:
            rmsf_file = os.path.join(self.rmsf_dir, str(temperature),
                                   f"{domain_id}_temperature_{temperature}_average_rmsf.csv")
        
        results["rmsf_file_exists"] = os.path.exists(rmsf_file)
        
        if not results["rmsf_file_exists"]:
            results["issues"].append(f"RMSF file not found: {rmsf_file}")
            return results
        
        # Count voxel files
        voxel_files = [f for f in os.listdir(domain_voxel_dir) if f.endswith('.hdf5')]
        results["voxel_files_count"] = len(voxel_files)
        
        if results["voxel_files_count"] == 0:
            results["issues"].append(f"No voxel files found in {domain_voxel_dir}")
            return results
        
        # Check voxel files for expected structure
        valid_voxel_files = 0
        residue_ids = set()
        
        for voxel_file in voxel_files:
            try:
                voxel_path = os.path.join(domain_voxel_dir, voxel_file)
                with h5py.File(voxel_path, 'r') as f:
                    # Check for required datasets
                    if "inputs" in f:
                        inputs = f["inputs"]
                        shape = inputs.shape
                        
                        # Check shape
                        if len(shape) >= 3:
                            valid_voxel_files += 1
                        else:
                            results["issues"].append(f"Invalid shape in {voxel_file}: {shape}")
                    
                    # Extract residue ID
                    resid = None
                    if "metadata" in f and "resid" in f["metadata"].attrs:
                        resid = f["metadata"].attrs["resid"]
                    elif "resid" in f.attrs:
                        resid = f.attrs["resid"]
                    
                    if resid is not None:
                        residue_ids.add(resid)
                    else:
                        results["issues"].append(f"Missing residue ID in {voxel_file}")
            except Exception as e:
                results["issues"].append(f"Error reading {voxel_file}: {str(e)}")
        
        results["residues_with_voxel_data"] = len(residue_ids)
        
        # Set overall validity
        results["is_valid"] = (
            results["voxel_dir_exists"] and
            results["rmsf_file_exists"] and
            valid_voxel_files > 0 and
            len(results["issues"]) == 0
        )
        
        return results
===== FILE: training/__init__.py =====
"""
Training module for ProtFlex.

This module provides utilities for training and evaluating protein flexibility prediction models.
"""

from .trainer import (
    RMSFTrainer,
    EarlyStopping,
    MetricTracker,
    create_optimizer,
    create_scheduler
)

===== FILE: utils/logging_utils.py =====
"""
Logging utilities for ProtFlex.

This module provides functions for setting up and using logging in the ProtFlex package.
"""

import os
import sys
import logging
from typing import Dict, Any, Optional
from datetime import datetime

def setup_logging(config: Dict[str, Any], override_level: Optional[int] = None) -> logging.Logger:
    """
    Set up logging based on configuration.

    Args:
        config: Configuration dictionary
        override_level: Override the log level from config

    Returns:
        Root logger instance
    """
    # Get log file path from config
    log_file = config["output"]["log_file"]
    log_dir = os.path.dirname(log_file)

    # Create log directory if it doesn't exist
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)

    # Determine log levels
    if override_level is not None:
        console_level = override_level
        file_level = override_level
    else:
        console_level = getattr(logging, config["logging"]["console_level"])
        file_level = getattr(logging, config["logging"]["file_level"])

    # Set root logger level to the most verbose of the two
    root_level = min(console_level, file_level)

    # Configure root logger
    logger = logging.getLogger()
    logger.setLevel(root_level)

    # Clear existing handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # Create formatters
    detailed_formatter = logging.Formatter(
        fmt='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_formatter = logging.Formatter(
        fmt='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )

    # Configure file handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(file_level)
    file_handler.setFormatter(detailed_formatter)
    logger.addHandler(file_handler)

    # Configure console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(console_level)
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    # Log initialization
    logger.info(f"Logging initialized at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"Log file: {log_file}")
    logger.debug(f"Console log level: {logging.getLevelName(console_level)}")
    logger.debug(f"File log level: {logging.getLevelName(file_level)}")

    return logger

def log_section(title: str, logger: Optional[logging.Logger] = None) -> None:
    """
    Log a section header to make log files more readable.

    Args:
        title: Section title
        logger: Logger instance (uses root logger if None)
    """
    if logger is None:
        logger = logging.getLogger()

    separator = "=" * (len(title) + 4)
    logger.info(f"\n{separator}\n  {title}  \n{separator}")

def log_config(config: Dict[str, Any], logger: Optional[logging.Logger] = None) -> None:
    """
    Log configuration parameters.

    Args:
        config: Configuration dictionary
        logger: Logger instance (uses root logger if None)
    """
    if logger is None:
        logger = logging.getLogger()

    log_section("Configuration", logger)

    # Log each section of the config
    for section_name, section in config.items():
        logger.info(f"{section_name}:")

        if isinstance(section, dict):
            for key, value in section.items():
                # Format the value for better readability
                if isinstance(value, dict):
                    logger.info(f"  {key}: <dict with {len(value)} items>")
                elif isinstance(value, list) and len(value) > 5:
                    logger.info(f"  {key}: <list with {len(value)} items>")
                else:
                    logger.info(f"  {key}: {value}")
        else:
            logger.info(f"  {section}")

def log_runtime_info(logger: Optional[logging.Logger] = None) -> None:
    """
    Log runtime information about the Python environment.

    Args:
        logger: Logger instance (uses root logger if None)
    """
    if logger is None:
        logger = logging.getLogger()

    import platform
    import torch

    log_section("Runtime Information", logger)

    logger.info(f"Python version: {platform.python_version()}")
    logger.info(f"Platform: {platform.platform()}")

    # PyTorch information
    logger.info(f"PyTorch version: {torch.__version__}")
    logger.info(f"CUDA available: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        logger.info(f"CUDA version: {torch.version.cuda}")
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    # Log CPU information
    try:
        import psutil
        logger.info(f"CPU count: {psutil.cpu_count(logical=True)} logical, {psutil.cpu_count(logical=False)} physical")
        mem_info = psutil.virtual_memory()
        logger.info(f"System memory: {mem_info.total / 1e9:.2f} GB total, {mem_info.available / 1e9:.2f} GB available")
    except ImportError:
        logger.info("CPU information not available (psutil not installed)")

def enable_verbose_logging() -> None:
    """
    Enable verbose logging for all loggers.
    """
    # Set root logger to DEBUG
    logging.getLogger().setLevel(logging.DEBUG)

    # Set all handlers to DEBUG
    for handler in logging.getLogger().handlers:
        handler.setLevel(logging.DEBUG)

    logging.debug("Verbose logging enabled")

def disable_logging() -> None:
    """
    Disable all logging except critical messages.
    """
    # Set root logger to CRITICAL
    logging.getLogger().setLevel(logging.CRITICAL)

    # Set all handlers to CRITICAL
    for handler in logging.getLogger().handlers:
        handler.setLevel(logging.CRITICAL)

===== FILE: utils/file_utils.py =====
"""
File handling utilities for ProtFlex.

This module provides functions for file operations and path manipulation.
"""

import os
import glob
import shutil
import tempfile
import h5py
import json
import yaml
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Union, Tuple
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

def ensure_dir(directory: str) -> str:
    """
    Ensure a directory exists, creating it if necessary.

    Args:
        directory: Directory path

    Returns:
        Absolute path to the directory
    """
    dir_path = os.path.abspath(os.path.expanduser(directory))
    os.makedirs(dir_path, exist_ok=True)
    return dir_path

def find_voxel_files(base_dir: str, domain_id: str, temperature: str) -> List[str]:
    """
    Find voxel files for a domain at a specific temperature.

    Args:
        base_dir: Base directory containing voxelized data
        domain_id: Domain identifier
        temperature: Temperature value

    Returns:
        List of voxel file paths
    """
    domain_dir = os.path.join(base_dir, domain_id, str(temperature))

    if not os.path.exists(domain_dir):
        logger.warning(f"Domain directory not found: {domain_dir}")
        return []

    voxel_files = glob.glob(os.path.join(domain_dir, "*.hdf5"))
    return voxel_files

def find_rmsf_file(rmsf_dir: str, domain_id: str, temperature: Union[int, str]) -> Optional[str]:
    """Find the RMSF file for a domain at a specific temperature."""
    # Try standard naming patterns
    if temperature == "average":
        # Try different naming conventions for average
        patterns = [
            f"{domain_id}_total_average_rmsf.csv",
            f"{domain_id}_average_rmsf.csv",
            f"{domain_id}_rmsf_average.csv"
        ]
        for pattern in patterns:
            rmsf_file = os.path.join(rmsf_dir, "average", pattern)
            if os.path.exists(rmsf_file):
                return rmsf_file
    else:
        # Try different naming conventions for specific temperature
        patterns = [
            f"{domain_id}_temperature_{temperature}_average_rmsf.csv",
            f"{domain_id}_temp{temperature}_average_rmsf.csv",
            f"{domain_id}_rmsf_{temperature}.csv"
        ]
        for pattern in patterns:
            rmsf_file = os.path.join(rmsf_dir, str(temperature), pattern)
            if os.path.exists(rmsf_file):
                return rmsf_file

    logger.warning(f"RMSF file not found for domain {domain_id} at temperature {temperature}")
    return None

def read_h5_metadata(h5_file: str) -> Dict[str, Any]:
    """
    Read metadata from an HDF5 file.

    Args:
        h5_file: Path to HDF5 file

    Returns:
        Dictionary of metadata
    """
    metadata = {}

    try:
        with h5py.File(h5_file, 'r') as f:
            # Read root attributes
            for key in f.attrs:
                metadata[key] = f.attrs[key]

            # Read metadata group if it exists
            if 'metadata' in f:
                metadata_group = f['metadata']
                for key in metadata_group.attrs:
                    metadata[f"metadata_{key}"] = metadata_group.attrs[key]

    except Exception as e:
        logger.error(f"Error reading metadata from {h5_file}: {e}")

    return metadata

def save_model_summary(model, output_file: str) -> None:
    """
    Save a summary of the model architecture to a file.

    Args:
        model: PyTorch model
        output_file: Path to output file
    """
    try:
        # Get model summary as string
        from io import StringIO
        import sys

        # Redirect stdout to capture summary
        original_stdout = sys.stdout
        sys.stdout = summary_io = StringIO()

        # Print model architecture
        print(model)

        # Print parameter count
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"\nTotal parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Non-trainable parameters: {total_params - trainable_params:,}")

        # Restore stdout
        sys.stdout = original_stdout

        # Write to file
        with open(output_file, 'w') as f:
            f.write(summary_io.getvalue())

        logger.info(f"Saved model summary to {output_file}")

    except Exception as e:
        logger.error(f"Error saving model summary: {e}")

def load_rmsf_data(rmsf_file: str) -> pd.DataFrame:
    """
    Load RMSF data from a CSV file.

    Args:
        rmsf_file: Path to RMSF CSV file

    Returns:
        DataFrame with RMSF data
    """
    try:
        df = pd.read_csv(rmsf_file)
        return df
    except Exception as e:
        logger.error(f"Error loading RMSF data from {rmsf_file}: {e}")
        return pd.DataFrame()

def save_to_json(data: Dict[str, Any], output_file: str) -> None:
    """
    Save data to a JSON file.

    Args:
        data: Data to save
        output_file: Path to output file
    """
    try:
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # Convert numpy/torch types to Python native types
        def convert_to_serializable(obj):
            if isinstance(obj, (np.integer, np.floating, np.bool_)):
                return obj.item()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (list, tuple)):
                return [convert_to_serializable(x) for x in obj]
            elif isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            return obj

        serializable_data = convert_to_serializable(data)

        # Write to file
        with open(output_file, 'w') as f:
            json.dump(serializable_data, f, indent=2)

        logger.info(f"Saved data to {output_file}")

    except Exception as e:
        logger.error(f"Error saving data to JSON: {e}")

def load_from_json(input_file: str) -> Dict[str, Any]:
    """
    Load data from a JSON file.

    Args:
        input_file: Path to input file

    Returns:
        Loaded data
    """
    try:
        with open(input_file, 'r') as f:
            data = json.load(f)
        return data
    except Exception as e:
        logger.error(f"Error loading data from {input_file}: {e}")
        return {}

def get_domain_ids(data_dir: str, voxel_dir: str, rmsf_dir: str,
                  temperature: Union[int, str]) -> List[str]:
    """
    Get list of domain IDs that have both voxel and RMSF data.

    Args:
        data_dir: Base data directory
        voxel_dir: Relative path to voxelized data
        rmsf_dir: Relative path to RMSF data
        temperature: Temperature value

    Returns:
        List of domain IDs
    """
    voxel_base_dir = os.path.join(data_dir, voxel_dir)
    rmsf_base_dir = os.path.join(data_dir, rmsf_dir)

    # Find domains with voxel data
    voxel_domains = set()
    if os.path.exists(voxel_base_dir):
        for item in os.listdir(voxel_base_dir):
            domain_dir = os.path.join(voxel_base_dir, item)
            if os.path.isdir(domain_dir):
                # Check if temperature directory exists
                temp_dir = os.path.join(domain_dir, str(temperature))
                if os.path.exists(temp_dir) and os.listdir(temp_dir):
                    voxel_domains.add(item)

    # Find domains with RMSF data
    rmsf_domains = set()
    if temperature == "average":
        rmsf_temp_dir = os.path.join(rmsf_base_dir, "average")
    else:
        rmsf_temp_dir = os.path.join(rmsf_base_dir, str(temperature))

    if os.path.exists(rmsf_temp_dir):
        for item in os.listdir(rmsf_temp_dir):
            if item.endswith(".csv"):
                if temperature == "average":
                    domain_id = item.replace("_total_average_rmsf.csv", "")
                else:
                    domain_id = item.split(f"_temperature_{temperature}")[0]
                rmsf_domains.add(domain_id)

    # Find domains with both voxel and RMSF data
    common_domains = voxel_domains.intersection(rmsf_domains)

    logger.info(f"Found {len(common_domains)} domains with both voxel and RMSF data for temperature {temperature}")

    return sorted(list(common_domains))

===== FILE: utils/__init__.py =====
"""
Utility module for ProtFlex.

This module provides various utility functions for file handling, logging, and visualization.
"""

from .logging_utils import setup_logging, log_section, log_config, log_runtime_info
from .file_utils import (
    ensure_dir,
    find_voxel_files,
    find_rmsf_file,
    get_domain_ids,
    load_rmsf_data,
    save_model_summary
)
from .visualization import (
    plot_loss_curves,
    plot_predictions,
    plot_error_distribution,
    plot_residue_analysis,
    plot_temperature_comparison,
    plot_feature_importance,
    plot_atom_channel_examples,
    plot_rmsf_profiles
)

===== FILE: utils/visualization.py =====
"""
Visualization utilities for ProtFlex.

This module provides functions for creating visualizations to analyze model performance
and protein flexibility predictions.
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Union, Any
from sklearn.metrics import mean_squared_error, r2_score

# Set plot style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_context("paper", font_scale=1.5, rc={"lines.linewidth": 2.5})

def plot_loss_curves(
    train_loss: List[float],
    val_loss: List[float],
    lr: Optional[List[float]] = None,
    output_path: Optional[str] = None,
    fig_size: Tuple[int, int] = (12, 6),
    dpi: int = 300
) -> None:
    """
    Plot training and validation loss curves with optional learning rate.
    
    Args:
        train_loss: List of training loss values
        val_loss: List of validation loss values
        lr: List of learning rates
        output_path: Path to save the figure
        fig_size: Figure size
        dpi: Figure resolution
    """
    epochs = list(range(1, len(train_loss) + 1))
    
    fig, ax1 = plt.subplots(figsize=fig_size)
    
    # Plot loss curves
    ax1.plot(epochs, train_loss, 'b-', label='Training Loss')
    ax1.plot(epochs, val_loss, 'r-', label='Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.tick_params(axis='y', labelcolor='black')
    
    # Add learning rate on secondary y-axis if provided
    if lr is not None:
        ax2 = ax1.twinx()
        ax2.plot(epochs, lr, 'g--', label='Learning Rate')
        ax2.set_ylabel('Learning Rate', color='g')
        ax2.tick_params(axis='y', labelcolor='g')
    
    # Add legend and title
    lines1, labels1 = ax1.get_legend_handles_labels()
    if lr is not None:
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
    else:
        ax1.legend(loc='upper right')
    
    plt.title('Loss and Learning Rate vs. Epoch')
    plt.tight_layout()
    
    # Save or show the plot
    if output_path:
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def plot_predictions(
    predictions: np.ndarray,
    targets: np.ndarray,
    output_path: Optional[str] = None,
    fig_size: Tuple[int, int] = (10, 10),
    dpi: int = 300,
    max_points: int = 1000
) -> None:
    """
    Create a scatter plot of predicted vs. actual RMSF values.
    
    Args:
        predictions: Array of predicted RMSF values
        targets: Array of actual RMSF values
        output_path: Path to save the figure
        fig_size: Figure size
        dpi: Figure resolution
        max_points: Maximum number of points to plot
    """
    # Calculate metrics
    mse = mean_squared_error(targets, predictions)
    rmse = np.sqrt(mse)
    r2 = r2_score(targets, predictions)
    
    # Limit the number of points for clearer visualization
    if len(predictions) > max_points:
        indices = np.random.choice(len(predictions), max_points, replace=False)
        predictions = predictions[indices]
        targets = targets[indices]
    
    # Create figure
    plt.figure(figsize=fig_size)
    
    # Create scatter plot
    scatter = plt.scatter(targets, predictions, alpha=0.6, s=20, c='blue')
    
    # Add identity line
    min_val = min(min(predictions), min(targets))
    max_val = max(max(predictions), max(targets))
    buffer = (max_val - min_val) * 0.05  # 5% buffer
    plt.plot([min_val - buffer, max_val + buffer], [min_val - buffer, max_val + buffer], 'r--')
    
    # Add metrics as text
    plt.text(
        0.05, 0.95, 
        f'RMSE: {rmse:.4f}\n$R^2$: {r2:.4f}',
        transform=plt.gca().transAxes,
        fontsize=14,
        verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8)
    )
    
    # Add labels and title
    plt.xlabel('Actual RMSF')
    plt.ylabel('Predicted RMSF')
    plt.title('Predicted vs. Actual RMSF Values')
    
    # Set equal aspect ratio
    plt.axis('equal')
    plt.grid(True, alpha=0.3)
    
    # Tight layout
    plt.tight_layout()
    
    # Save or show the plot
    if output_path:
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def plot_error_distribution(
    predictions: np.ndarray,
    targets: np.ndarray,
    output_path: Optional[str] = None,
    fig_size: Tuple[int, int] = (12, 8),
    dpi: int = 300
) -> None:
    """
    Plot the distribution of prediction errors.
    
    Args:
        predictions: Array of predicted RMSF values
        targets: Array of actual RMSF values
        output_path: Path to save the figure
        fig_size: Figure size
        dpi: Figure resolution
    """
    # Calculate errors
    errors = predictions - targets
    abs_errors = np.abs(errors)
    
    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=fig_size)
    
    # Plot histograms
    sns.histplot(errors, bins=30, kde=True, ax=ax1)
    ax1.set_xlabel('Prediction Error (Predicted - Actual)')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Distribution of Prediction Errors')
    
    # Add error statistics
    mean_error = np.mean(errors)
    std_error = np.std(errors)
    median_error = np.median(errors)
    
    ax1.text(
        0.05, 0.95,
        f'Mean: {mean_error:.4f}\nStd Dev: {std_error:.4f}\nMedian: {median_error:.4f}',
        transform=ax1.transAxes,
        fontsize=12,
        verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8)
    )
    
    # Plot error vs. actual value
    scatter = ax2.scatter(targets, abs_errors, alpha=0.6, s=10, c='blue')
    ax2.set_xlabel('Actual RMSF')
    ax2.set_ylabel('Absolute Error')
    ax2.set_title('Absolute Error vs. Actual RMSF')
    
    # Add a trend line using polynomial regression
    z = np.polyfit(targets, abs_errors, 1)
    p = np.poly1d(z)
    x_line = np.linspace(min(targets), max(targets), 100)
    ax2.plot(x_line, p(x_line), 'r--', linewidth=2)
    
    # Add correlation coefficient
    corr = np.corrcoef(targets, abs_errors)[0, 1]
    ax2.text(
        0.05, 0.95,
        f'Correlation: {corr:.4f}',
        transform=ax2.transAxes,
        fontsize=12,
        verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8)
    )
    
    # Tight layout
    plt.tight_layout()
    
    # Save or show the plot
    if output_path:
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def plot_residue_analysis(
    predictions: np.ndarray,
    targets: np.ndarray,
    residue_types: List[str],
    output_path: Optional[str] = None,
    fig_size: Tuple[int, int] = (14, 10),
    dpi: int = 300
) -> None:
    """
    Analyze prediction performance by residue type.
    
    Args:
        predictions: Array of predicted RMSF values
        targets: Array of actual RMSF values
        residue_types: List of residue types (3-letter codes)
        output_path: Path to save the figure
        fig_size: Figure size
        dpi: Figure resolution
    """
    # Calculate errors
    errors = predictions - targets
    abs_errors = np.abs(errors)
    
    # Create a DataFrame for analysis
    df = pd.DataFrame({
        'Residue': residue_types,
        'Actual': targets,
        'Predicted': predictions,
        'Error': errors,
        'AbsError': abs_errors
    })
    
    # Create figure with multiple subplots
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=fig_size)
    
    # 1. Mean Absolute Error by Residue Type
    residue_mae = df.groupby('Residue')['AbsError'].mean().sort_values(ascending=False)
    residue_mae.plot(kind='bar', ax=ax1)
    ax1.set_xlabel('Residue Type')
    ax1.set_ylabel('Mean Absolute Error')
    ax1.set_title('Mean Absolute Error by Residue Type')
    
    # 2. Mean RMSF by Residue Type (Actual vs. Predicted)
    residue_means = df.groupby('Residue').agg({
        'Actual': 'mean',
        'Predicted': 'mean'
    }).sort_values(by='Actual', ascending=False)
    
    residue_means.plot(kind='bar', ax=ax2)
    ax2.set_xlabel('Residue Type')
    ax2.set_ylabel('Mean RMSF')
    ax2.set_title('Mean RMSF by Residue Type (Actual vs. Predicted)')
    
    # 3. Boxplot of Absolute Errors by Residue Type
    # Get the top 10 residues with highest mean absolute errors
    top_residues = residue_mae.head(10).index.tolist()
    sns.boxplot(x='Residue', y='AbsError', data=df[df['Residue'].isin(top_residues)], ax=ax3)
    ax3.set_xlabel('Residue Type')
    ax3.set_ylabel('Absolute Error')
    ax3.set_title('Distribution of Absolute Errors for Top 10 Residues')
    
    # 4. Correlation between Actual RMSF and Error
    for residue in df['Residue'].unique():
        residue_df = df[df['Residue'] == residue]
        ax4.scatter(residue_df['Actual'], residue_df['AbsError'], alpha=0.5, label=residue)
    
    # Add a trend line for all data
    z = np.polyfit(df['Actual'], df['AbsError'], 1)
    p = np.poly1d(z)
    x_line = np.linspace(min(df['Actual']), max(df['Actual']), 100)
    ax4.plot(x_line, p(x_line), 'r--', linewidth=2)
    
    ax4.set_xlabel('Actual RMSF')
    ax4.set_ylabel('Absolute Error')
    ax4.set_title('Absolute Error vs. Actual RMSF by Residue Type')
    
    # Handle the legend - only show a subset if there are many residue types
    if len(df['Residue'].unique()) > 6:
        ax4.legend().set_visible(False)
    else:
        ax4.legend(loc='upper right')
    
    # Tight layout
    plt.tight_layout()
    
    # Save or show the plot
    if output_path:
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def plot_temperature_comparison(
    temperatures: List[int],
    metrics: Dict[int, Dict[str, float]],
    output_path: Optional[str] = None,
    fig_size: Tuple[int, int] = (12, 8),
    dpi: int = 300
) -> None:
    """
    Compare model performance across different temperatures.
    
    Args:
        temperatures: List of temperature values
        metrics: Dictionary mapping temperatures to metrics
        output_path: Path to save the figure
        fig_size: Figure size
        dpi: Figure resolution
    """
    # Extract metrics
    mse_values = [metrics[temp].get('mse', 0) for temp in temperatures]
    mae_values = [metrics[temp].get('mae', 0) for temp in temperatures]
    r2_values = [metrics[temp].get('r2', 0) for temp in temperatures]
    
    # Create figure with multiple subplots
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=fig_size)
    
    # Plot MSE by temperature
    ax1.plot(temperatures, mse_values, 'o-', linewidth=2, markersize=8)
    ax1.set_xlabel('Temperature (K)')
    ax1.set_ylabel('Mean Squared Error')
    ax1.set_title('MSE vs. Temperature')
    ax1.grid(True, alpha=0.3)
    
    # Plot MAE by temperature
    ax2.plot(temperatures, mae_values, 'o-', linewidth=2, markersize=8, color='orange')
    ax2.set_xlabel('Temperature (K)')
    ax2.set_ylabel('Mean Absolute Error')
    ax2.set_title('MAE vs. Temperature')
    ax2.grid(True, alpha=0.3)
    
    # Plot R² by temperature
    ax3.plot(temperatures, r2_values, 'o-', linewidth=2, markersize=8, color='green')
    ax3.set_xlabel('Temperature (K)')
    ax3.set_ylabel('R² Score')
    ax3.set_title('R² vs. Temperature')
    ax3.grid(True, alpha=0.3)
    
    # Tight layout
    plt.tight_layout()
    
    # Save or show the plot
    if output_path:
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def plot_feature_importance(
    feature_names: List[str],
    importance_scores: np.ndarray,
    output_path: Optional[str] = None,
    fig_size: Tuple[int, int] = (10, 8),
    dpi: int = 300
) -> None:
    """
    Plot feature importance scores.
    
    Args:
        feature_names: List of feature names
        importance_scores: Array of importance scores
        output_path: Path to save the figure
        fig_size: Figure size
        dpi: Figure resolution
    """
    # Create a DataFrame for plotting
    df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importance_scores
    }).sort_values(by='Importance', ascending=False)
    
    # Create figure
    plt.figure(figsize=fig_size)
    
    # Create bar plot
    sns.barplot(x='Importance', y='Feature', data=df, palette='viridis')
    
    # Add labels and title
    plt.xlabel('Importance Score')
    plt.ylabel('Feature')
    plt.title('Feature Importance')
    
    # Tight layout
    plt.tight_layout()
    
    # Save or show the plot
    if output_path:
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def plot_atom_channel_examples(
    voxel_data: np.ndarray,
    channel_names: List[str] = ['C', 'N', 'O', 'CA', 'CB'],
    slice_idx: int = None,
    output_path: Optional[str] = None,
    fig_size: Tuple[int, int] = (15, 3),
    dpi: int = 300
) -> None:
    """
    Visualize atom channel slices from a voxel grid.
    
    Args:
        voxel_data: Voxel grid data with shape [channels, depth, height, width]
        channel_names: Names of the channels
        slice_idx: Index of the slice to visualize (default: middle slice)
        output_path: Path to save the figure
        fig_size: Figure size
        dpi: Figure resolution
    """
    num_channels = voxel_data.shape[0]
    depth = voxel_data.shape[1]
    
    # If no slice index is provided, use the middle slice
    if slice_idx is None:
        slice_idx = depth // 2
    
    # Create figure
    fig, axes = plt.subplots(1, num_channels, figsize=fig_size)
    
    # Plot each channel
    for i in range(num_channels):
        channel_slice = voxel_data[i, slice_idx, :, :]
        
        # Plot the slice
        im = axes[i].imshow(channel_slice, cmap='viridis')
        axes[i].set_title(f'{channel_names[i]} Atoms')
        axes[i].axis('off')
        
        # Add colorbar
        fig.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)
    
    # Add overall title
    plt.suptitle(f'Voxel Grid Slice (z={slice_idx})', fontsize=16)
    
    # Tight layout
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    
    # Save or show the plot
    if output_path:
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def plot_rmsf_profiles(
    domain_id: str,
    residue_ids: List[int],
    actual_rmsf: List[float],
    predicted_rmsf: List[float],
    residue_names: Optional[List[str]] = None,
    secondary_structure: Optional[List[str]] = None,
    output_path: Optional[str] = None,
    fig_size: Tuple[int, int] = (12, 6),
    dpi: int = 300
) -> None:
    """
    Plot actual and predicted RMSF profiles for a protein domain.
    
    Args:
        domain_id: Protein domain identifier
        residue_ids: List of residue IDs
        actual_rmsf: List of actual RMSF values
        predicted_rmsf: List of predicted RMSF values
        residue_names: List of residue names (optional)
        secondary_structure: List of secondary structure assignments (optional)
        output_path: Path to save the figure
        fig_size: Figure size
        dpi: Figure resolution
    """
    # Create figure
    fig, ax = plt.subplots(figsize=fig_size)
    
    # Plot RMSF profiles
    ax.plot(residue_ids, actual_rmsf, '-', label='Actual RMSF', color='blue')
    ax.plot(residue_ids, predicted_rmsf, '--', label='Predicted RMSF', color='red')
    
    # Add secondary structure bars if provided
    if secondary_structure is not None:
        # Initialize secondary structure regions
        ss_regions = []
        current_ss = secondary_structure[0]
        start_idx = residue_ids[0]
        
        for i, ss in enumerate(secondary_structure[1:], 1):
            if ss != current_ss:
                ss_regions.append((current_ss, start_idx, residue_ids[i-1]))
                current_ss = ss
                start_idx = residue_ids[i]
        
        # Add the last region
        ss_regions.append((current_ss, start_idx, residue_ids[-1]))
        
        # Plot secondary structure as horizontal bars
        y_pos = min(actual_rmsf + predicted_rmsf) - 0.1
        height = 0.05
        
        ss_colors = {
            'H': 'red',       # Alpha helix
            'G': 'orange',    # 3-10 helix
            'I': 'yellow',    # Pi helix
            'E': 'blue',      # Extended strand
            'B': 'purple',    # Beta bridge
            'T': 'green',     # Turn
            'S': 'pink',      # Bend
            'C': 'gray',      # Coil
            'L': 'gray'       # Loop
        }
        
        for ss, start, end in ss_regions:
            color = ss_colors.get(ss, 'gray')
            ax.axhspan(y_pos, y_pos + height, xmin=(start - residue_ids[0])/(residue_ids[-1] - residue_ids[0]),
                     xmax=(end - residue_ids[0])/(residue_ids[-1] - residue_ids[0]), color=color, alpha=0.3)
        
        # Add legend for secondary structure
        import matplotlib.patches as mpatches
        ss_patches = []
        for ss, color in ss_colors.items():
            if any(region[0] == ss for region in ss_regions):
                ss_patches.append(mpatches.Patch(color=color, alpha=0.3, label=f'{ss}'))
        
        # Add the secondary structure legend below the main legend
        ax.legend(handles=ss_patches, loc='upper right', bbox_to_anchor=(1, 0.5), title='Secondary Structure')
    
    # Add labels and title
    ax.set_xlabel('Residue ID')
    ax.set_ylabel('RMSF')
    ax.set_title(f'RMSF Profile for Domain {domain_id}')
    
    # Add the main legend
    ax.legend(loc='upper right')
    
    # Annotate residue names if provided and not too many
    if residue_names is not None and len(residue_ids) <= 50:
        for i, (res_id, res_name) in enumerate(zip(residue_ids, residue_names)):
            if i % 5 == 0:  # Annotate every 5th residue to avoid crowding
                ax.text(res_id, max(actual_rmsf[i], predicted_rmsf[i]) + 0.05, res_name, 
                       ha='center', fontsize=8, rotation=90)
    
    # Grid and tight layout
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # Save or show the plot
    if output_path:
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

=======================================
Extracting First 10 Lines from Data21 (Ignoring Binary Files)
=======================================

